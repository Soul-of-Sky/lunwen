%!TEX root = ../main.tex

\chapter{快照系统架构设计}
\label{ch:design}

在第二章对主流虚拟化技术及其内存快照机制进行深入的原理性分析之后，本章将遵循严谨的系统工程设计范式，提出一种面向资源受限环境的高度优化快照系统架构。本章首先通过资源约束的数学建模，精确界定系统设计的边界条件与性能瓶颈；其次，对传统脏页追踪机制在异构、低算力平台上的性能衰退现象进行深度机制剖析；最后，核心性地提出基于热点感知与资源反馈的 HPRO（Hotspot Perception and Resource Optimization）分层系统架构，并对其中内核监控层、策略决策层以及执行优化层的逻辑功能与协同机制进行详尽阐述，旨在为后续的实现与性能验证提供精确的顶层设计蓝图。

\section{边缘虚拟化环境下的关键资源约束建模}
\label{sec:constraint}

与高性能数据中心服务器的资源充裕度形成鲜明对比，边缘计算节点通常部署在网络末端，其核心特征在于物理资源高度受限，同时必须承担实时控制、本地 AI 推理、数据预处理及通信聚合等多项关键任务。为了实现对这种复杂环境的定量化、工程化分析，本研究首先建立了轻量级虚拟化平台下的资源约束数学模型。

假设一个具有代表性的边缘虚拟化节点 $N$ 可以被抽象为以下四个关键资源集合 $R_{node}$：
$$R_{node} = \{ C_{total}, M_{total}, B_{io}, E_{bat} \}$$
其中，各项参数的物理意义和约束条件如下：

\begin{itemize}
	\item $C_{total}$（总计算能力）：代表中央处理单元（CPU）的总有效计算能力。对于常采用 ARM 或 RISC-V 架构的边缘片上系统（SoC），其设计目标侧重于能效比，故其单核主频和核心数量（例如，通常为 2-4 个异构核心）均显著低于服务器级处理器，这意味着其单位时间内的处理能力是关键瓶颈。

	\item $M_{total}$（物理内存总量）：表示系统可用的物理内存容量。边缘设备通常配置在 GB 级别的容量，这对 Host OS、Hypervisor 以及多个 Guest VM 的内存分配提出了精细化管理的要求，以避免内存资源过度竞争。

	\item $B_{io}$（最大持续 I/O 带宽）：表示存储子系统（通常为 NAND Flash 或 eMMC）的最大持续写入速率。受限于 Flash 介质的擦写延迟及控制器性能的物理约束，该带宽远低于数据中心采用的 NVMe SSD，是快照持久化过程中的主要性能限制因素。

	\item $E_{bat}$（能源状态）：对于部署在移动或野外环境的节点，电池容量和能耗效率是不可忽略的关键设计维度。快照任务的额外能耗必须纳入系统的总体功耗预算。
\end{itemize}

当系统执行内存状态快照任务 $T_{snap}$ 时，其资源消耗和时延必须严格满足上层业务所规定的服务质量（Quality of Service, QoS）要求。这构成了本研究系统设计的三大硬性约束：

\textbf{（1）计算资源竞争约束：低开销（Low Overhead）特性}

快照执行过程中涉及的内存页扫描、数据压缩、数据校验等步骤本质上是计算密集型任务。为了防止快照后台进程对核心业务进程 $P_{workload}$（如实时控制环路或 AI 实时推理）造成“资源饥饿”，必须对快照任务的 CPU 占用率 $C_{snap}$ 进行严格的上界限制。数学表达为：
$$C_{snap}(t) + \sum P_{workload}(t) \le C_{total}, \quad \forall t$$
此约束要求所设计的快照机制必须具备极低开销的特性，避免因频繁的上下文切换或CPU 时间片抢占而导致实时业务发生时序违约（Timing Violation）。

\textbf{（2）I/O 带宽非阻塞约束：精细化流量整形}

快照的核心操作是将 Guest VM 的内存状态数据持久化至存储介质，从而产生高密度的写入流 $b_{snap}$。若不对其进行精细控制，该写入流极易瞬时占满总线带宽 $B_{io}$，导致关键业务的 I/O 请求 $b_{workload}$ 遭受队头阻塞，严重恶化业务响应延迟。因此，需要预留一个安全带宽阈值 $\delta$：
$$b_{snap}(t) + b_{workload}(t) \le B_{io} \times (1 - \delta)$$
这一约束要求快照系统必须集成精细的流量整形与动态流控机制，确保在进行数据持久化的同时，业务 I/O 的最小响应能力得以保障。

\textbf{（3）停机时间与实时性约束：最小化服务不可用窗口}

对于对延迟极为敏感的工业物联网（IIoT）或自动驾驶场景，快照在一致性点所导致的服务不可用窗口$D_{stop}$ 必须被严格限制在业务容忍的阈值 $T_{max}$ 之内：$$D_{stop} \le T_{max}$$任何超出此阈值的服务停顿都可能被系统判定为故障，甚至在安全关键系统中引发事故。为便于后续章节的算法推导与机制分析，表 \ref{tab:resource_constraints} 汇总了本章建模所定义的关键符号及其对应的物理含义与系统约束要求。

\begin{table}[htbp]
    \centering
    \caption{边缘节点资源约束模型符号定义与设计要求}
    \label{tab:resource_constraints}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{l l l l}
        \toprule
        \textbf{符号} & \textbf{物理含义} & \textbf{资源特性} & \textbf{设计约束要求} \\
        \midrule
        $C_{total}$ & 
        总计算能力 & 
        单核主频低 & 
        \textbf{低开销} (防止业务饥饿) \\
        
        $B_{io}$ & 
        存储写入带宽 & 
        Flash 写入慢 & 
        \textbf{非阻塞} (预留安全阈值 $\delta$) \\
        
        $D_{stop}$ & 
        服务停机时间 & 
        业务敏感度高 & 
        \textbf{强实时} (需满足 $D_{stop} \le T_{max}$) \\
        
        $M_{total}$ & 
        物理内存总量 & 
        容量受限 & 
        \textbf{低占用} (避免额外消耗) \\
        \bottomrule
    \end{tabular}
\end{table}

综上所述，本研究所设计的 HPRO 系统的核心优化目标，即是在严格满足上述三大资源约束条件 $S.T.$ 的前提下，通过创新的算法与架构设计，最小化快照的总耗时 $T_{duration}$ 和存储空间占用 $S_{storage}$，以实现系统的高可用性与资源低消耗之间的最优工程平衡。


\section{传统快照机制在受限环境下的失效分析}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/conflict.png}
    \caption{边缘环境下的资源竞争与快照失效模型}
    \label{fig:conflict}
\end{figure}

现有的主流虚拟机快照机制，包括停止并拷贝（Stop-and-Copy）、预拷贝（Pre-copy）和后拷贝（Post-copy），其设计基础均建立在服务器级硬件资源充裕性的假设之上。当这些机制被直接应用于资源受限的边缘计算节点时，由于总计算能力 $C_{total}$ 和存储持续 I/O 带宽 $B_{io}$ 的显著降低，其在面对写密集型应用负载时将出现严重的机制性失效。本节将结合 \ref{sec:constraint} 节所建立的资源约束模型，对这些传统机制的局限性进行定量与定性分析。图\ref{fig:conflict}直观展示了传统快照机制在边缘受限资源下的失效模式：预拷贝引发的I/O带宽拥塞导致了迭代不收敛，而后拷贝引发的计算资源争抢则导致了严重的业务抖动，两者均无法满足边缘环境的硬性约束。

\subsection{停止并拷贝机制与停机时间约束的冲突}

停止并拷贝机制采用一种“先暂停，后传输”的策略来保证快照时点的一致性。在此机制下，虚拟机的服务不可用窗口（停机时间）$D_{stop}$ 是一个关键指标，其近似表达式如下：$$D_{stop} \approx \frac{M_{vm}}{B_{io}}$$其中，$M_{vm}$ 表示虚拟机的总内存大小。在边缘计算环境下，存储子系统受限于 NAND Flash 介质及其控制器，导致 $B_{io}$ 极其有限，持续写入速率通常仅为数十兆字节每秒（MB/s）量级。因此，对于一个 GB 级别的虚拟机内存状态 $M_{vm}$，其 $D_{stop}$ 将达到数十秒甚至更长的时间，这远超实时业务所允许的阈值 $T_{max}$（通常要求在毫秒级）。故停止并拷贝机制直接违背了 $D_{stop} \le T_{max}$ 的停机时间约束，在对延迟和实时性要求较高的边缘场景中，被判定为不可行方案。

\subsection{预拷贝机制在低带宽约束下的收敛性失效}

预拷贝机制通过在虚拟机持续运行期间进行迭代式脏页传输，来压缩最终停机阶段需要拷贝的数据量，从而实现对 $D_{stop}$ 的优化\cite{clark2005live}.该机制有效性的先决条件是内存传输速率 $R_{trans}$ 必须持续大于内存脏页产生速率 $R_{dirty}$ \cite{hines2009post}：$$
R_{trans} > R_{dirty}
$$
然而，在边缘计算环境中，以下两个因素使得收敛性假设极易被打破：传输速率受限：受限于边缘节点的 $B_{io}$ 约束，快照的数据传输速率 $R_{trans}$ 处于较低水平。脏页速率较高：AI 推理、数据处理等写密集型计算任务会伴随高频的内存写入操作，导致 $R_{dirty}$ 处于较高水平。当出现 $R_{dirty} > R_{trans}$ 的情况时，根据迭代收敛理论，每轮迭代后剩余的脏页数量不降反升，导致算法陷入不收敛状态\cite{liu2011performance}.为了防止快照过程无限期延长，系统最终必须强制进入长时间停机阶段以完成剩余数据的拷贝，这实际上退化为停止并拷贝，重新引入了高 $D_{stop}$ 问题。此外，在长时间的迭代过程中，持续进行的脏页追踪和数据传输将对 $B_{io}$ 和 $C_{total}$ 造成持续占用，违反了 I/O 带宽非阻塞约束与计算资源竞争约束，对前台业务性能造成显著且持久的负面影响。


\subsection{后拷贝机制对计算资源竞争约束的违背}

后拷贝（Post-copy）机制作为一种优化停机时间 $D_{stop}$ 的策略，通过将内存状态的实际数据传输推迟至虚拟机恢复运行之后，成功地将 $D_{stop}$ 压缩至极小值。然而，这种策略的代价是将数据恢复的压力转化为对系统运行时资源的瞬时高竞争负荷。在资源受限的边缘计算节点上，这一机制会直接诱发严重的计算资源竞争，导致直接违反 \ref{sec:constraint} 节所定义的计算约束。

具体而言，后拷贝机制的核心工作流程依赖于缺页异常机制来实现数据的写时复制。尽管在算力充裕的服务器级平台上，处理缺页异常所产生的 CPU 开销可以被视为相对可忽略的次要因素；但在主频较低、流水线较浅的边缘处理器上，处理一次缺页涉及昂贵的 VM Exit/VM Entry 以及宿主机侧内核态（KVM）与用户态（QEMU）之间的上下文切换。

在虚拟机恢复运行的初期，由于业务进程需要快速重建其内存工作集，这将诱发大量的缺页异常并发请求，形成所谓的“缺页风暴”\cite{hines2009post}.在此状态下，系统大量的 CPU 周期将被消耗在操作系统内核的异常处理路径以及虚拟化层的模式切换上。这导致快照恢复过程的计算消耗 $C_{snap}(t)$ 在短时间内急剧激增\cite{liu2011performance}.根据 \ref{sec:constraint} 节定义的计算资源竞争约束模型：
$$
C_{snap}(t) + \sum P_{workload}(t) \le C_{total}, \quad \forall t
$$
瞬时激增的 $C_{snap}$ 必然导致核心业务进程 $P_{workload}$ 可获得的 CPU 时间片被严重挤占。这种现象使得处理器处于一种计算高负载但有效吞吐量低的“伪忙碌”状态，并且直接导致前台业务出现剧烈的性能抖动与响应超时，从而无法满足边缘场景下对服务延迟稳定性和实时性的严格要求。因此，后拷贝机制在本质上是以牺牲运行时计算资源的稳定性来换取停机时间的缩短，在资源受限节点上是不可接受的策略。


\section{基于热度感知与资源反馈的 HPRO 系统架构}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/hpro.png}
    \caption{HPRO 系统总体架构}
    \label{fig:hpro}
\end{figure}

针对上述传统机制在低 $C_{total}$ 和低 $B_{io}$ 边缘环境下的固有失效问题，本研究提出了一种分层感知、闭环反馈的系统架构——HPRO。HPRO 架构的核心目标是通过细粒度资源感知和动态策略调度，在严格满足 \ref{sec:constraint} 节所定义的约束条件下，寻求快照总耗时与资源开销之间的最优工程解。HPRO 系统架构在逻辑上分为三个相互协作的层次：内核监控层、策略决策层与执行优化层。如图 \ref{fig:hpro} 所示，该架构构建了从底层硬件脏位感知，到用户态共享内存通信，再到闭环资源决策与I/O执行的完整数据通路，实现了对边缘环境异构资源的自适应管理。

\textbf{（1）内核监控层}

内核监控层驻留于 KVM 宿主机（Host OS）的内核空间，是 HPRO 架构中负责状态感知和原始数据采集的基石。该层的首要设计目标是实现透明化、超低开销的信息捕获，以避免引入新的性能瓶颈。其核心在于实施的轻量级内存状态感知机制，该机制创新性地利用现代处理器页表项中由硬件自动维护的脏位替代传统的写保护陷出机制进行脏页追踪。此举通过将事件驱动的高频中断转化为时间驱动的低频轮询操作，从根本上消除了写保护机制所导致的 VM Exit 开销，显著降低了监控过程对 $C_{total}$ 的持续和突发占用，为后续的决策提供了可靠、低扰动的数据源。此外，为消除用户态（QEMU）与内核态（KVM）之间传统通信的数据拷贝和模式切换瓶颈，本层创造性地引入了共享内存机制，将内核中维护的实时热度位图直接映射到用户态 QEMU 的地址空间。通过结合原子操作和 RCU（Read-Copy-Update）等精细的并发控制技术，实现了内核线程与用户态决策模块对该位图的无锁并发访问，从而确保策略决策层能够以亚毫秒级的极低延迟获取最新的内存写入的热度信息，保证了闭环控制的时效性。

\textbf{（2）策略决策层}

策略决策层运行于用户空间，是 HPRO 系统的控制论核心模块和自适应调度引擎，其功能在于将底层采集的原始、低级数据转化为高层次、有约束保障的执行指令和动态资源控制参数。该层首先基于内核监控层上报的脏位序列，通过实施自适应多位老化算法，对内存写入的时空局部性进行精确建模。该算法不仅量化了页面的写入频率，还将内存页面划分为多级热度类别（如极热页、温热页、冷寂页），从而实现了对当前最小化核心工作集的精确动态界定。在此基础上，本层制定并输出分级差异化传输的策略指令：定义对不同冷热页面（冷寂页、温热页、极热页）应采取的 I/O 优先级和传输时机。同时，本层定义并实时计算系统压力指数（System Pressure Index, SPI），该指数是综合 $C_{total}$ 负载、I/O 队列深度及内存水位等多个实时维度信息后归一化处理得到的量化指标，准确反映了边缘计算节点的整体资源紧张程度。SPI 的变化曲线构成系统级的闭环反馈信号，直接指导快照执行的自适应状态机，并在 “业务优先模式” 与 “快照优先模式” 之间进行智能、动态的切换，输出给执行优化层作为 I/O 带宽流控的速率参数，确保快照执行始终严格服从 \ref{sec:constraint} 节所规定的计算资源竞争约束和 I/O 带宽非阻塞约束。

\textbf{（3）执行优化层}

执行优化层是位于数据持久化路径末端的数据搬迁与存储适配模块，其职责是严格按照策略层的指令，将内存数据高效、安全、并以对存储介质友好的方式持久化。该层实现分级差异化传输引擎，执行策略层定义的 I/O 策略：对冷寂页和温热页，执行低优先级后台异步传输以平摊 I/O 负载；而对极热页（内存修改频率最高的页面），则通过预先的写合并处理减少重复写入，或将其传输推迟至最终的极短停机窗口内完成，从而在保证数据一致性的前提下，将 $D_{stop}$ 优化到最低。为应对 NAND Flash 介质的固有物理缺陷，尤其是其对随机写入的性能惩罚和由此引发的写放大效应，本层集成了基于环形缓冲区的写合并机制，负责将策略层输出的离散逻辑块写入请求在缓冲区内汇聚并优化为顺序的大块物理写入。同时，配合细粒度数据去重方案，进一步减少了冗余数据的写入，有效延长了存储介质的使用寿命。此外，本层还部署了基于令牌桶的反馈式流控机制，该机制利用策略决策层提供的 SPI 指数动态调整令牌桶的速率参数，对快照 I/O 带宽进行精细的流量整形，强制确保 $b_{snap}$ 始终保持在安全阈值 $\delta$ 之内，彻底根除了因快照瞬时突发性写入操作导致的业务 I/O 阻塞问题，从而保障了系统的 I/O 稳定性。

\section{本章小结}

本章首先建立了边缘虚拟化环境下的多维资源约束模型，确立了快照系统设计的理论边界。在此基础上，深入分析了全量拷贝、预拷贝及后拷贝三种传统机制在受限环境下的失效根源，指出其根本矛盾在于无法在低带宽与弱算力条件下平衡停机时间与资源占用。针对这一问题，提出了 HPRO 系统总体架构。该架构通过内核级低开销监控、基于资源反馈的动态决策以及面向存储特性的执行优化，构建了一个能够自适应边缘环境约束的高可用快照系统，为后续章节的关键技术实现提供了顶层框架。



