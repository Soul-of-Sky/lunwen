% !TEX root = ../main.tex

\chapter{实验评估与结果分析}
\label{ch:eval}

本章旨在构建一套严谨且具有代表性的实验框架，依托真实的边缘计算硬件平台，对所提出的 HPRO 系统进行系统性的性能评估。这一评估的核心目的在于全面量化 HPRO 系统在资源受限场景下，尤其是在计算能力、内存容量和存储带宽均存在约束的边缘环境中，所展现出的性能特征与优化效能。实验设计围绕四个关键的性能指标维度展开，具体包括：停机时间、业务性能损失、存储开销以及系统适应性。

在实验过程中，我们将聚焦于对 HPRO 系统内部核心机制的有效性进行验证。这主要包括：第一，对本文提出的轻量级热点感知机制进行细致的检验，论证其在不显著增加系统运行负担的前提下，准确识别和追踪关键状态的能力；第二，评估基于热度感知和资源反馈的动态快照策略，探究该策略如何根据边缘环境的动态变化和业务负载的实时需求，优化快照过程的资源分配，从而有效降低停机时间和性能抖动；第三，验证面向 Flash 的存储优化方案，旨在证明该方案在利用非易失性存储（如 $\text{NAND}$ Flash）特性时，如何最小化存储写入放大效应，降低对存储介质的磨损，并加速快照数据的持久化与加载过程。

随后，为了确立 HPRO 系统的优势，我们将引入一系列对比实验。这些对比实验将在多种不同的工作负载环境下进行，旨在全面模拟边缘计算环境中多样化的业务场景。对比对象主要包括：主流虚拟机管理程序中的原生快照平台（ $\text{QEMU}$ 的原生快照机制）和同类型的优化系统（ $\text{MLLM}$ 和 $\text{FLIC}$ ）。通过在同一硬件平台和统一评估标准下的系统性对比，本章将有力地论证 HPRO 系统在确保虚拟机或容器化应用数据一致性、最大化服务连续性以及实现资源高效利用等方面的显著优势。该论证将为 HPRO 系统的实用价值和技术创新性提供坚实的定量依据。

\section{实验环境与配置}

为了确保本研究的实验结果具备高度的现实意义、场景代表性与科学可复现性，本研究精心构建了一个典型的嵌入式虚拟化实验平台。为确保测试集的覆盖范围能够紧密契合边缘计算的前沿应用趋势，特别是近年来激增的边缘人工智能应用场景，本研究除了设计传统的计算密集型、$\text{I/O}$ 密集型及混合型负载之外，还特意纳入了边缘 $\text{AI}$ 相关工作负载。这些 $\text{AI}$ 负载的引入，旨在模拟实际边缘设备如智能监控、自动驾驶辅助系统或工业物联网实时分析中，对系统资源提出的独特和严苛要求。

\subsection{硬件与软件环境}

本研究选用两款在边缘计算领域具有代表性的开发板作为宿主机（Host）：

\begin{itemize}
  \item \textbf{Raspberry Pi 4 Model B（主测试平台）}：作为高性能边缘节点的代表，该平台用于关键技术微基准测试及大部分宏观性能评估。其核心搭载 $\text{Broadcom BCM2711 SoC}$（四核 $\text{Cortex-A72}$，主频 $\text{1.5GHz}$）。虽然板载 $\text{4GB LPDDR4}$ 内存，但为了模拟苛刻的资源受限环境，我们通过内核启动参数 mem=2G 将其可用内存严格限制在 $\text{2GB}$。
  \item \textbf{Raspberry Pi 3 Model B+（辅助测试平台）}：为了在宏观基准测试中覆盖更广泛的边缘应用场景，并验证系统在极低算力与内存受限环境下的适应性，本研究引入了该平台。其搭载 $\text{Broadcom BCM2837B0 SoC}$（四核 $\text{Cortex-A53}$，主频 $\text{1.4GHz}$），仅配备 $\text{1GB LPDDR2}$ 内存。该平台主要用于验证 $\text{HPRO}$ 在极度受限硬件上的通用性与鲁棒性。
\end{itemize}

两种平台均采用 $\text{32GB Class 10 MicroSD}$ 卡作为主存储介质。由于 $\text{MicroSD}$ 卡具备 $\text{NAND Flash}$ 典型的物理特性（如显著的读写非对称性和随机写入性能衰减），被特意用于验证第 $6$ 章所提出的面向 $\text{Flash}$ 的存储优化机制的有效性。

软件栈层面，两款宿主机均运行稳定的 $\text{Ubuntu Server 20.04 LTS}$（内核版本 $\text{5.4.0}$）。虚拟化层基于主流的 $\text{QEMU 6.2.0}$ 配合 $\text{KVM}$ 实现硬件辅助虚拟化。客户机被配置为 $\text{2 vCPU}$。内存分配根据宿主机能力动态调整：在 $\text{Pi 4B}$ 上分配 $\text{1GB}$，在 $\text{Pi 3B+}$ 上分配 $\text{512MB}$。

操作系统方面，对于通用负载，客户机运行极简的 $\text{Alpine Linux 3.18}$；鉴于 $\text{Alpine}$ 的 $\text{musl-libc}$ 库与 $\text{TensorFlow}$ 框架存在兼容性问题，针对 $\text{YOLO}$ 和 $\text{TinyLlama}$ 等 $\text{AI}$ 推理负载（仅在 $\text{Pi 4B}$ 上运行），采用 $\text{Ubuntu 20.04}$ 作为客户机操作系统。

\subsection{实验负载}

参考边缘场景下的典型应用 ，本研究选取了以下负载进行测试：

\begin{enumerate}
  \item \textbf{空闲}：仅运行操作系统基础守护进程，用于测定系统底噪。
  \item \textbf{SQLite（轻量级数据库）}：执行频繁的插入与更新操作，产生大量随机 I/O，用于验证存储子系统的处理能力。
  \item \textbf{7zip（数据压缩）}：产生持续的高内存带宽占用，用于评估在高频脏页产生场景下系统的承载能力。
  \item \textbf{OpenCV（计算机视觉处理）}：运行图像处理与特征提取任务，用于模拟中等强度的计算与内存访问混合型负载。
  \item \textbf{YOLO（实时目标检测）}：执行实时推理任务，具有高 CPU/GPU 占用与连续内存写入特征，用于测试快照操作对计算密集型 AI 工作负载的影响。
  \item \textbf{TinyLlama（轻量级大语言模型推理）}：运行小型 Transformer 模型的推理任务，用于模拟典型的内存访问密集与中等计算开销的 LLM 工作负载。
\end{enumerate}

此外，考虑到 $\text{Raspberry Pi 3B+}$ 等内存受限设备无法支持 $\text{YOLO}$ 和 $\text{TinyLlama}$ 等高负载应用，本研究引入了以下两种替代负载，以覆盖中低端嵌入式平台的应用需求：

\begin{enumerate}
    \setcounter{enumi}{7} % 接上面的编号
    \item \textbf{MQTT} \cite{hillar2017mqtt}：利用轻量级消息协议进行通信，模拟物联网设备间的实时消息传递场景。
    \item \textbf{Lighttpd} \cite{bogus2008lighttpd}：运行轻量级 Web 服务器，模拟低流量 HTTP 请求的网络通信，适用于 IoT 设备的本地服务。
\end{enumerate}

\subsection{对比基准}

在宏观基准测试性能中，本研究选取以下三种快照机制作为对比基准：  

\begin{enumerate}

\item \textbf{QEMU}：基于 QEMU 提供的原生预拷贝（pre-copy）机制。
\item \textbf{MLLS}：基于 MLLM 提出的内存写入工作集感知迁移方法的快照机制。
\item \textbf{FLIC-DRAM}：基于 FLIC 提出的工作集识别方法进行优化的快照机制。

\end{enumerate}

\section{关键技术微基准测试}

本节通过隔离测试，分别验证 HPRO 架构中热点感知机制、动态快照策略和存储与I/O优化三个核心模块的性能贡献。

\subsection{热点感知机制的性能评估}

在本节中，我们旨在对 $\text{HPRO}$ 系统中集成的热点感知算法的性能进行量化评估。由于精确的工作集识别是整个优化策略的中枢神经，其准确性直接决定了快照的效率。考虑到工作集与快照过程之间存在紧密的动态耦合关系——理想的工作集应当仅包含那些在后台保存期间极有可能被修改的页面。然而，在实际运行中，保存耗时与工作集大小互为因果，使得“精确定义”变得极具挑战性。因此，基于前文的先验分析，我们采取了一种更为直观且工程有效的判定标准：将写入频率排名前 $5\%$ 且写入次数大于 $1$ 的页面视为核心工作集。为了全面衡量 $\text{HPRO}$ 的优势，我们将本研究提出的热点感知算法与经典的 LRU 缓存替换算法进行了横向对比测试。测试覆盖了六种典型的嵌入式应用场景，旨在验证不同负载特征下的识别精度。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/acc.pdf}
    \caption{HPRO 与 LRU 算法的工作集识别准确率对比}
    \label{fig:acc}
\end{figure}

如图 \ref{fig:acc} 所示，实验结果有力地证明了 $\text{HPRO}$ 在所有测试场景下的工作集识别精度均显著优于传统的 $\text{LRU}$ 算法。在资源受限且访问模式复杂的边缘环境中，$\text{LRU}$ 往往因无法区分“一次性扫描”与“周期性热点”而导致缓存污染，而 $\text{HPRO}$ 凭借自适应的老化机制展现出了更强的鲁棒性。具体而言，在AI 推理任务中，$\text{HPRO}$ 的表现尤为亮眼。在 $\text{TinyLlama}$ 大模型推理场景下，$\text{HPRO}$ 实现了高达 $92.5\%$ 的识别准确率，相比 $\text{LRU}$ 提升了 $5.8\%$。这一数据表明，$\text{HPRO}$ 能够敏锐捕捉到神经网络推理过程中权重更新与中间张量计算产生的特定内存访问热度，而这些特征往往被单纯基于时间局部性的 $\text{LRU}$ 所忽略。在更为复杂的数据密集型任务中，优势进一步扩大。对于 $\text{SQLite}$ 数据库负载，由于其涉及频繁的 $\text{B-Tree}$ 索引更新与日志写入，访问模式极具跳跃性，$\text{HPRO}$ 相比 $\text{LRU}$ 实现了 $7.0\%$ 的精度提升；同样，在 $\text{7zip}$ 高压缩比计算场景下，精度也提升了 $5.9\%$。这些结果不仅验证了 $\text{HPRO}$ 在动态多变负载下精准锁定关键页面的能力，更凸显了其在处理 AI 推理及高资源消耗型任务时的核心优势，为后续的快照优化奠定了坚实的数据基础。

\subsection{热点动态变化的适应性评估}

本节旨在评估 $\text{HPRO}$ 系统中热点感知算法对工作负载动态变化即热页分布漂移的反应速度和适应能力。这种适应性是衡量算法在实际、多变边缘环境中鲁棒性的关键指标。不同于前文关注快照执行的整体性能，本实验剥离了快照传输过程，专注于监测算法在系统运行过程中对热页集合动态更新的响应灵敏度。实验设计了一个典型的边缘智能流水线突变场景：客户机的工作负载从内存密集型的 $\text{OpenCV}$ 图像处理任务（表现为基于矩阵的图像变换与特征提取）动态切换到计算密集型的 $\text{YOLO}$ 实时目标检测（表现为对大块连续内存缓冲区的高频更新）。评估的重点集中于在工作负载切换发生时，热点检测精度随检测周期变化的行为模式。通过对比 $\text{HPRO}$ 算法与 $\text{LRU}$ 方法在应对 $\text{OpenCV}$ 到 $\text{YOLO}$ 这种跨度极大的负载转换时，检测精度恢复到稳定高水平所需的时间和精度曲线的平稳性，来严格评估 $\text{HPRO}$ 算法在处理热页动态转换方面的有效性和优越性。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/shift.pdf}
    \caption{工作负载切换下的热点检测精度变化}
    \label{fig:adaptability}
\end{figure}

如图 \ref{fig:adaptability} 所示，实验结果清晰地描绘了本研究提出的 $\text{HPRO}$ 热点感知算法与传统基于 LRU 的方法在面对工作负载动态切换时，其热点检测精度随时间推移而发生的瞬态变化与恢复过程。在初始的 $\text{OpenCV}$ 稳定负载阶段，两种算法均能保持在较高的准确性水平，体现了它们在静态负载环境下的有效性。然而，当系统经历从 $\text{OpenCV}$ 到 $\text{YOLO}$ 的工作负载转换并引发大规模热点页面集合的动态迁移时，两种算法的检测精度均不可避免地出现了显著的瞬时下降。关键的优势体现在随后的恢复阶段：本研究提出的 $\text{HPRO}$ 算法展现出卓越的自适应能力和响应速度，能够在 $5$ 个采样间隔内迅速地将检测精度恢复至可靠且稳定的高准确度水平；与之形成鲜明对比的是，传统的 $\text{LRU}$ 技术则需要更长的周期，耗费 $8$ 个采样间隔才能达到与之相当的准确度状态。这主要归因于 LRU 算法固有的"扫描污染”（Scan Pollution）弱点。当工作负载切换至 YOLO 时，AI 推理过程涉及大量的顺序内存读取（如模型加载与特征图生成），这些一次性访问的页面迅速占据了 LRU 列表的头部，掩盖了真正的稳定热点。系统需要消耗多个检测周期来通过淘汰机制清洗这些瞬态页面。相比之下，HPRO 采用的多位老化算法引入了频率维度。它要求页面在多个连续周期内保持活跃才能被判定为热点，这种机制天然具有抗噪声能力，能够迅速过滤掉 YOLO 初始化阶段的一次性内存访问，从而在 5 个周期内快速收敛至高精度状态。这一关键差异有力地证明了 $\text{HPRO}$ 算法在处理边缘计算环境中常见的瞬时高变化性工作负载时的优越性，确保了系统在动态调整过程中能够更快地恢复对关键资源的精确感知与高效管理。

\subsection{基于 SPI 反馈的快照自适应模式切换机制评估}

本节旨在验证 $\text{HPRO}$ 系统中策略决策层的核心组件——基于系统压力指数（$\text{SPI}$）的自适应状态机在动态资源约束下的调度有效性。实验重点评估该机制能否根据实时计算的 $\text{SPI}$ 值（综合了 $\text{CPU}$ 负载、$\text{I/O}$ 队列深度及内存水位），在低压模式、标准模式与高压模式之间进行准确且平滑的切换，以在保障前台业务服务质量（$\text{QoS}$）的同时最大化快照效率。实验构建了一个剧烈波动的混合负载场景：在 $\text{TinyLlama}$ 推理任务（计算密集）与 $\text{SQLite}$ 读写事务（$\text{I/O}$ 密集）之间进行周期性叠加与释放，使得宿主机的资源利用率在 $\text{10\%}$ 至 $\text{95\%}$ 之间大幅震荡。我们将 $\text{HPRO}$ 的自适应策略与两种静态基准策略进行了对比：一是激进策略，即始终全速执行快照传输；二是保守策略，即始终开启严格的令牌桶限流。评估的核心指标包括前台业务的尾延迟（99th percentile latency，即 99\% 请求的响应时延不超过该值）以及快照完成的总耗时，旨在验证迟滞比较器在防止策略震荡方面的作用以及 $\text{SPI}$ 反馈对突发负载的削峰能力。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/spi.pdf}
    \caption{不同负载压力下的快照模式自适应切换与业务延迟对比}
    \label{fig:spi_eval}
\end{figure}

如图 \ref{fig:spi_eval} 所示，实验结果直观地展示了 $\text{HPRO}$ 系统随 $\text{SPI}$ 曲线波动而动态调整执行模式的全过程，及其对业务延迟的显著优化效果。在实验初期的低负载阶段（$\text{0s - 20s}$，$\text{SPI} < 0.3$），系统正确识别出资源充裕状态并驻留在低压模式，解除了后台传输带宽限制。此时 $\text{HPRO}$ 的快照传输速率与激进策略持平，显著高于保守策略，有效利用了闲置带宽。随着 $\text{TinyLlama}$ 推理任务的并发度增加，系统负载急剧攀升（$\text{20s - 40s}$，$\text{SPI}$ 激增至 $\text{0.85}$）。此时，激进策略由于持续抢占 $\text{CPU}$ 时间片，导致前台业务延迟飙升至正常水平的 $\text{4.5}$ 倍，出现严重的 $\text{QoS}$ 违约；而 $\text{HPRO}$ 系统迅速触发高阈值，切换至高压模式，主动暂停了后台脏页扫描与非关键 $\text{I/O}$。结果显示，在峰值负载期间，$\text{HPRO}$ 将业务延迟控制在基准值的 $\text{1.15}$ 倍以内，与保守策略表现相当，成功实现了对关键业务的让路。值得注意的是，在负载回落阶段（$\text{40s - 60s}$），得益于迟滞比较器的引入，系统并未在阈值附近频繁跳变，而是平稳过渡至标准模式，启动令牌桶流控进行均衡传输。综合全过程数据，$\text{HPRO}$ 在保证业务尾延迟仅增加 $\text{8\%}$ 的前提下，相比保守策略缩短了 $\text{37\%}$ 的快照总耗时，证明了该机制在复杂边缘环境下实现了资源利用率与业务稳定性的最优平衡。

\subsection{基于 SPI 的连续快照动态增量策略评估}

本节旨在对 $\text{HPRO}$ 系统中提出的基于 $\text{SPI}$ 的连续快照动态增量策略进行系统性评估。与单次快照不同，连续快照要求系统在长时间运行中持续平衡快照频率与系统性能损耗这对核心矛盾。为了验证策略在动态负载下的适应性，我们构建了一个梯度压力测试场景：在固定 $\text{2}$ 分钟的快照间隔内，将计算密集型任务 $\text{TinyLlama}$ 的负载占比从 $\text{0\%}$（完全空闲）逐步提升至 $\text{100\%}$（全速推理），以此模拟边缘设备从待机到满负荷运行的资源状态演变。为了量化策略行为，我们采用了可用性指数作为核心指标，其定义为 $\log_{10} \left( 100 \cdot \frac{N_{\text{flush}}}{N_{\text{dirty}}} \right)$。该指标直观地反映了脏数据的落盘激进程度：值为 $\text{2}$ 代表写时复制（$\text{COW}$）级别的实时落盘；值降至 $\text{0}$ 以下则代表高聚合度的懒惰写入。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/cont.pdf}
    \caption{连续快照策略的可用性与批处理粒度变化}
    \label{fig:cont_eval}
\end{figure}

如图 \ref{fig:cont_eval} 所示，实验数据定量地展示了 $\text{HPRO}$ 在不同资源压力下的自适应调度特征：在低负载甚至空闲状态（0\% 负载）下，$\text{SPI}$ 指数处于极低水平，系统判定资源充裕，此时可用性指数维持在峰值 2.0（对应原始落盘率 100\%），系统采用标准写时复制（COW）模式，充分利用闲置 I/O 带宽对每一次内存写入进行实时捕获，从而实现毫秒级的恢复点目标（RPO）。当引入约 10\% 的 TinyLlama 背景负载后，$\text{HPRO}$ 仍维持较高落盘频率（可用性指数约为 1.35），仅执行轻度的批处理聚合策略，以优先保障数据一致性。随着负载提升至中等区间（20\%–50\%），CPU 与内存带宽的竞争逐步加剧，$\text{HPRO}$ 的策略引擎基于 SPI 的变化进行动态干预。实验结果呈现出显著的负相关趋势：可用性指数由 0.75 快速下降至 0.23，而平均 I/O 批处理粒度则由 17 页扩展至 59 页。该现象表明系统通过主动降低实时落盘频率，以减少 I/O 中断次数，从而缓解快照线程在业务繁忙阶段对计算资源的争用。当系统进入资源饱和状态（100\% 负载）时，可用性指数进一步降至约 –0.52（对应原始落盘率约 0.3\%），$\text{HPRO}$ 切换至高聚合度的延迟写入模式，大量脏页被保留在环形缓冲区中进行逻辑合并。尽管该策略会增加数据落地时延，但能够有效保障前台推理任务的性能稳定性，避免强制 I/O 带来的业务阻塞风险。综合来看，$\text{HPRO}$ 能够随系统负载梯度实现平滑而连续的策略迁移，在资源受限且负载波动剧烈的边缘环境中展现出良好的稳定性与鲁棒性。

\subsection{面向 Flash 特性的存储与 I/O 优化评估}

本节旨在验证 $\text{HPRO}$ 系统在执行优化层针对 $\text{Flash}$ 存储介质物理特性所设计的优化机制的有效性。实验重点关注两个维度：一是基于环形缓冲区的写合并机制在降低写放大与 $\text{I/O}$ 频率方面的表现；二是基于指纹库的全页去重与细粒度提取技术在缩减快照镜像体积方面的贡献。

\subsubsection{基于环形缓冲区的写合并机制效能}

在边缘嵌入式设备中，$\text{NAND Flash}$ 的“读写不对称”与“异地更新”特性导致其在面对随机小块写入时性能急剧下降。本实验通过运行 $\text{SQLite}$ 数据库基准测试（高频随机写入负载），对比了原生 $\text{QEMU}$ 快照与开启写合并机制后的 $\text{HPRO}$ 系统在物理 $\text{I/O}$ 提交次数（$\text{IOPS}$）上的差异。我们监测了快照期间下发至块设备驱动层的实际写入请求数量，以评估环形缓冲区对逻辑写入流的整形效果。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/ring.pdf}
    \caption{写合并机制对物理 I/O 提交次数的优化对比}
    \label{fig:write_coalescing}
\end{figure}

如图 \ref{fig:write_coalescing} 所示，实验结果表明基于环形缓冲区的写合并机制显著降低了底层存储的写入压力。在 $\text{SQLite}$ 负载的高频更新阶段，原生 $\text{QEMU}$ 机制产生的物理 $\text{IOPS}$ 几乎与逻辑写入请求保持线性一致，导致存储总线迅速饱和。相比之下，$\text{HPRO}$ 利用环形缓冲区在内存中拦截了大量的中间态覆写操作，并将剩余的离散脏页聚合成大块顺序写入。数据显示，在峰值区间，$\text{HPRO}$ 将物理 $\text{I/O}$ 提交次数减少了约 $\text{78\%}$。这种“逻辑覆写、物理合并”的策略不仅大幅缓解了 $\text{Flash}$ 的写放大效应（$\text{WA}$），还有效降低了因频繁垃圾回收（$\text{GC}$）引发的系统卡顿风险，验证了该机制在延长存储寿命与提升写入吞吐方面的双重优势。


\subsubsection{基于指纹库的全页去重效能}

在边缘计算节点存储空间（$S_{storage}$）高度受限的背景下，$\text{HPRO}$ 系统引入了基于指纹库的全页去重机制。全页去重主要针对多实例部署场景下存在的跨虚拟机（$\text{Inter-VM}$）数据冗余。为了量化这一机制的收益，我们在 $\text{Raspberry Pi 4B}$ 宿主机上部署了 $\text{1}$ 至 $\text{4}$ 个运行相同 $\text{Alpine Linux}$ 基础镜像的客户机，并同时触发快照操作。这些虚拟机共享相同的内核代码段、系统库文件（如 $\text{libc.so}$）以及应用二进制文件，理论上存在大量内容完全一致的物理内存页。

实验记录了在不同并发虚拟机数量下，快照数据的原始总量与经全局指纹库去重后的实际存储量，统计结果如表 \ref{tab:deduplication} 所示。

\begin{table}[htbp]
    \centering
    \caption{多虚拟机场景下基于指纹库的全页去重效果对比}
    \label{tab:deduplication}
    \begin{tabular}{ccccc}
        \toprule
        \textbf{虚拟机数量} & \textbf{原始数据总量} & \textbf{去重后存储量} & \textbf{节省空间} & \textbf{去重率} \\
        \midrule
        1 & 1024 MB & 315 MB & 709 MB & 69.2\% \\
        2 & 2048 MB & 342 MB & 1706 MB & 83.3\% \\
        3 & 3072 MB & 378 MB & 2694 MB & 87.7\% \\
        4 & 4096 MB & 412 MB & 3684 MB & 89.9\% \\
        \bottomrule
    \end{tabular}
\end{table}


如表 \ref{tab:deduplication} 所示，全页去重机制在多实例场景下展现出了极高的存储效率。在单虚拟机场景下，去重率主要来源于零页过滤与内核自身的页内冗余，达到 $\text{69.2\%}$。随着虚拟机数量的增加，去重率呈现显著上升趋势。当部署 $\text{4}$ 个同构虚拟机时，尽管原始内存状态总量高达 $\text{4096 MB}$，但实际占用的存储空间仅为 $\text{412 MB}$，去重率攀升至 $\text{89.9\%}$。这表明新增虚拟机的绝大部分内存状态（主要是只读代码段和未修改的堆栈初始化数据）都能在全局指纹库中找到命中项，系统仅需存储极少量的差异化私有数据。这一结果有力地证明了 $\text{HPRO}$ 的全页去重机制能够有效消除跨机冗余，极大缓解了边缘设备在承载高密度容器或虚拟机时的存储压力。

\subsubsection{细粒度提取的最佳粒度维度分析}

在全页去重的基础上，为了进一步挖掘页内微小修改带来的压缩潜力，$\text{HPRO}$ 引入了细粒度提取技术。正如第 6 章所述，更细的提取粒度能够剥离页面中未发生变化的“干净”数据块，从而减少无效写入。然而，过细的粒度会带来额外的元数据索引开销与 $\text{CPU}$ 比对成本。因此，本实验旨在通过定量分析确定最佳的粒度维度，在不同粒度设置下对快照脏数据进行差异化提取，并计算其压缩比率。

\begin{table}[htbp]
    \centering
    \caption{不同提取粒度下的脏数据体积占比 (\%)}
    \label{tab:granularity}
    \setlength{\tabcolsep}{10pt} % 增加列间距，使表格更舒展
    \begin{tabular}{lcccc}
        \toprule
        \textbf{负载场景} & \textbf{64 B} & \textbf{256 B} & \textbf{1024 B} & \textbf{4096 B (全页)} \\
        \midrule
        Idle       & 5.45  & 13.69 & 37.65 & 100.0 \\
        SQLite           & 93.68 & 95.47 & 97.24 & 100.0 \\
        OpenCV           & 96.67 & 98.48 & 99.41 & 100.0 \\
        YOLO             & 28.00 & 36.31 & 55.84 & 100.0 \\
        TinyLlama        & 23.95 & 37.03 & 61.53 & 100.0 \\
        7zip             & 94.31 & 95.44 & 96.96 & 100.0 \\
        \bottomrule
    \end{tabular}
\end{table}

如表 \ref{tab:granularity} 所示，实验结果清晰地展示了脏数据压缩比随粒度细化而变化的非线性趋势。当提取粒度从标准的 $\text{4096B}$ 缩减至 $\text{1024B}$，并进一步细化至 $\text{256B}$ 时，所有负载下的压缩比均获得了显著提升。这表明在大多数应用场景中，内存的“脏化”行为往往具有局部性，即 $\text{4KB}$ 页面中通常只有少部分字节被实际修改。然而，当粒度从 $\text{256B}$ 继续减小至 $\text{64B}$ 时，曲线趋于平缓，压缩比的提升幅度明显收窄，表现出显著的边际效应递减特征。考虑到粒度过小会导致位图索引体积激增并加重 $\text{CPU}$ 负担，$\text{256B}$ 被确认为最佳的工程折衷点。基于此结论，$\text{HPRO}$ 系统最终采用 $\text{256B}$ 作为细粒度提取的标准单元，在实现高效数据缩减的同时，避免了过度的系统开销，实现了存储收益与计算成本的最优平衡。

\subsection{宏观基准测试性能}

本节旨在通过构建复杂的嵌入式应用场景，对 $\text{HPRO}$ 系统进行全方位的宏观评估，以验证其在真实、动态且资源受限的工作负载下的性能表现与运行效率。基于第 2 章中对边缘计算环境资源约束的深入分析，我们确立了快照性能评估指标的优先级排序，由高到低依次为：虚拟机性能损耗（直接影响前台实时业务的服务质量 QoS）、虚拟机停机时间（决定了系统的可用性与服务中断窗口）以及快照总耗时（反映了资源占用的持续窗口）。

为了确保评估的客观性与全面性，我们将 $\text{HPRO}$ 的性能与以下三个具有代表性的系统进行了严格的横向对比：（1）\textbf{QEMU}：$\text{QEMU 6.2.0}$ 版本中基于软件写保护的原生预拷贝（Pre-copy）机制，作为通用基准；（2）\textbf{MLLS}：一种源自 $\text{MLLM}$ \cite{shi2022memory} 的优化机制，其特点是利用内存写入工作集感知（Working Set Awareness）来进行迁移决策；（3）\textbf{FLIC-DRAM}：基于 $\text{FLIC}$ \cite{zhong2016flic} 工作集识别方法进行优化的快照系统。通过这三组对比，我们将分别验证 $\text{HPRO}$ 在低开销追踪、热点识别准确性以及整体架构优化方面的优势。

\subsubsection{虚拟机性能损耗}

首先，我们重点评估 $\text{HPRO}$ 在执行快照过程中对虚拟机前台业务造成的性能损耗。这是边缘场景最为敏感的指标，因为过高的损耗会导致实时任务超时或 $\text{AI}$ 推理掉帧。实验在两种不同算力的平台上进行：在高性能的 $\text{Raspberry Pi 4B}$ 上运行了 $\text{SQLite}$（数据库）、$\text{7zip}$（高频内存修改）、$\text{OpenCV}$（图像处理）和 $\text{YOLO}$（AI 推理）负载；在资源更受限的 $\text{Raspberry Pi 3B+}$ 上运行了 $\text{SQLite}$、$\text{7zip}$、$\text{OpenCV}$ 和 $\text{MQTT}$（轻量级通信）负载。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/pl4b.pdf}
    \caption{Raspberry Pi 4B 平台上虚拟机快照引入的性能损失}
    \label{fig:pl4b}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/pl3b.pdf}
    \caption{Raspberry Pi 3B+ 平台上虚拟机快照引入的性能损失}
    \label{fig:pl3b}
\end{figure}

如图~\ref{fig:pl4b} 和图~\ref{fig:pl3b} 的量化数据所示，在 $\text{Raspberry Pi 4B}$ 平台上，相比于 $\text{QEMU}$、$\text{MLLS}$ 和 $\text{FLIC-DRAM}$，$\text{HPRO}$ 系统表现出显著的优越性，分别平均降低了 $\text{37.2\%}$、$\text{14.5\%}$ 和 $\text{24.2\%}$ 的虚拟机性能损耗（损耗计算公式为 \((x-\text{HPRO})/x\)）。在算力更弱的 $\text{Raspberry Pi 3B+}$ 上，这一优势进一步扩大，$\text{HPRO}$ 相比上述三个系统分别平均降低了 $\text{44.7\%}$、$\text{18.8\%}$ 和 $\text{34.2\%}$ 的性能损耗。

特别值得深入分析的是在 $\text{7zip}$ 这种典型的写密集型负载下，$\text{HPRO}$ 对性能损耗的降低幅度高达 $\text{65.2\%}$。这一现象背后的核心原因在于底层追踪机制的本质差异：$\text{QEMU}$ 和 $\text{MLLS}$ 等传统机制依赖于页表写保护（Write Protection），高频的内存写入会触发海量的 $\text{EPT Violation}$ 异常，迫使 $\text{CPU}$ 频繁陷入内核态进行处理，严重抢占了业务进程的时间片。相反，$\text{HPRO}$ 利用硬件辅助脏位技术替代了软件陷阱，实现了非侵入式的脏页追踪；配合基于 $\text{SPI}$ 的自适应流控机制，$\text{HPRO}$ 能够智能地避开业务负载高峰，避免在脏页生成最为剧烈的时刻进行无效的反复拷贝与总线争抢。这些结果有力证明了在资源受限且脏页生成频繁的边缘场景中，$\text{HPRO}$ 通过精确的热点识别与被动式追踪策略，成功将快照操作对前台计算任务的干扰降至最低。

\subsubsection{虚拟机停机时间}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/dt4b.pdf}
    \caption{Raspberry Pi 4B 平台上的虚拟机快照停机时间}
    \label{fig:dt4b}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/dt3b.pdf}
    \caption{Raspberry Pi 3B+ 平台上的虚拟机快照停机时间}
    \label{fig:dt3b}
\end{figure}

接下来，我们评估决定服务可用性的关键指标——虚拟机停机时间。实验设置与负载配置详见前文“实验环境与配置”小节。如图~\ref{fig:dt4b} 和图~\ref{fig:dt3b} 所示，在 $\text{Raspberry Pi 4B}$ 平台上，相比于原生预拷贝（$\text{Pre-copy}$）机制和基于 $\text{FLIC}$ 算法的优化方案，$\text{HPRO}$ 的停机时间分别平均减少了 $\text{14.2\%}$ 和 $\text{12.7\%}$。这表明 $\text{HPRO}$ 的热点感知算法能够更准确地收敛剩余脏页集，从而缩短最后时刻的传输窗口。

数据对比中一个值得注意的细节是，与 $\text{MLLS}$ 系统相比，$\text{HPRO}$ 的停机时间平均微幅增加了 $\text{7.5\%}$。通过深入分析快照流程可以发现，这是设计哲学权衡下的合理结果：$\text{MLLS}$ 采用了极其激进的预拷贝策略，在在线阶段就尝试传输绝大部分工作集，导致其停机时仅需传输极少量页面。然而，这种策略在 $\text{Pre-copy}$ 阶段引入了过高的开销，且对工作负载类型高度敏感——一旦负载波动导致脏页率上升，其停机时间会发生剧烈抖动。相比之下，$\text{HPRO}$ 选择将“极热页”推迟至停机阶段集中处理，虽然停机时间略有增加，但换取了极高的确定性与稳定性。

此外，硬件平台的差异进一步验证了 $\text{HPRO}$ 的鲁棒性。在 $\text{Raspberry Pi 3B+}$ 上，$\text{HPRO}$ 相比预拷贝和 $\text{FLIC-DRAM}$ 分别减少了 $\text{15.4\%}$ 和 $\text{10.7\%}$ 的停机时间。更为关键的是，在该平台上，$\text{HPRO}$ 与 $\text{MLLS}$ 的停机时间差距被大幅缩小，仅平均增加了 $\text{1.3\%}$。这种差异缩小的根本原因在于：当计算资源极其有限（如 $\text{3B+}$ 的单核性能较弱）时，$\text{MLLS}$ 复杂的在线迁移算法无法跟上脏页的产生速率，导致其激进策略失效，停机时间被迫延长。而 $\text{HPRO}$ 凭借轻量级的老化算法和写合并机制，即使在低端硬件上也能保持高效的收敛能力，证明了其在极端受限环境下的适应能力优于对比系统。

\subsubsection{快照总耗时}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/du4b.pdf}
    \caption{Raspberry Pi 4B 平台上的虚拟机快照持续时间}
    \label{fig:du4b}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/py/du3b.pdf}
    \caption{Raspberry Pi 3B+ 平台上的虚拟机快照持续时间}
    \label{fig:du3b}
\end{figure}

最后，我们评估快照过程持续的总时长，该指标反映了快照任务占用系统 $\text{I/O}$ 和网络资源的“时间窗口”长度。实验结果如图~\ref{fig:du4b} 和图~\ref{fig:du3b} 所示，在 $\text{Raspberry Pi 4B}$ 平台上，$\text{HPRO}$ 展现出卓越的传输效率，相比 $\text{QEMU}$、$\text{MLLS}$ 和 $\text{FLIC-DRAM}$ 分别平均缩短了 $\text{26\%}$、$\text{7.5\%}$ 和 $\text{7.7\%}$ 的快照持续时间。

在硬件配置更低的 $\text{Raspberry Pi 3B+}$ 平台上，优化效果甚至更为显著，$\text{HPRO}$ 相比上述三个系统分别平均缩短了 $\text{28.8\%}$、$\text{6.9\%}$ 和 $\text{8.8\%}$ 的耗时。快照总耗时的大幅缩短，主要得益于 $\text{HPRO}$ 在执行层引入的多层级数据去重与细粒度提取技术，以及面向 $\text{Flash}$ 特性的写合并机制。这些技术有效减少了实际写入存储介质的物理数据量，缓解了 $\text{Flash}$ 的写入放大效应，从而在带宽有限的情况下实现了更高的数据吞吐率。这一系列数据充分证明了 $\text{HPRO}$ 架构具有良好的跨平台通用性，无论是在相对高性能的边缘网关还是极低功耗的嵌入式节点上，均能通过软硬协同的优化策略，显著提升系统整体的运行效率。