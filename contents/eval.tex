% !TEX root = ../main.tex

\chapter{实验评估与结果分析}
\label{ch:eval}

本章旨在构建一套严谨且具有代表性的实验框架，依托真实的边缘计算硬件平台，对所提出的 HPRO 系统进行系统性的性能评估。这一评估的核心目的在于全面量化 HPRO 系统在资源受限场景下，尤其是在计算能力、内存容量和存储带宽均存在约束的边缘环境中，所展现出的性能特征与优化效能。实验设计围绕四个关键的性能指标维度展开，具体包括：停机时间、业务性能损失、存储开销以及系统适应性。

在实验过程中，我们将聚焦于对 HPRO 系统内部核心机制的有效性进行验证。这主要包括：第一，对本文提出的轻量级热点感知机制进行细致的检验，论证其在不显著增加系统运行负担的前提下，准确识别和追踪关键状态的能力；第二，评估基于热度感知和资源反馈的动态快照策略，探究该策略如何根据边缘环境的动态变化和业务负载的实时需求，优化快照过程的资源分配，从而有效降低停机时间和性能抖动；第三，验证面向 Flash 的存储优化方案，旨在证明该方案在利用非易失性存储（如 $\text{NAND}$ Flash）特性时，如何最小化存储写入放大效应，降低对存储介质的磨损，并加速快照数据的持久化与加载过程。

随后，为了确立 HPRO 系统的优势，我们将引入一系列对比实验。这些对比实验将在多种不同的工作负载环境下进行，旨在全面模拟边缘计算环境中多样化的业务场景。对比对象主要包括：主流虚拟机管理程序中的原生快照平台（ $\text{QEMU}$ 的原生快照机制）和同类型的优化系统（ $\text{MLLM}$ 和 $\text{FLIC}$ ）。通过在同一硬件平台和统一评估标准下的系统性对比，本章将有力地论证 HPRO 系统在确保虚拟机或容器化应用数据一致性、最大化服务连续性以及实现资源高效利用等方面的显著优势。该论证将为 HPRO 系统的实用价值和技术创新性提供坚实的定量依据。

\section{实验环境与配置}

为了确保本研究的实验结果具备高度的现实意义、场景代表性与科学可复现性，本研究精心构建了一个典型的嵌入式虚拟化实验平台。为确保测试集的覆盖范围能够紧密契合边缘计算的前沿应用趋势，特别是近年来激增的边缘人工智能应用场景，本研究除了设计传统的计算密集型、$\text{I/O}$ 密集型及混合型负载之外，还特意纳入了边缘 $\text{AI}$ 相关工作负载。这些 $\text{AI}$ 负载的引入，旨在模拟实际边缘设备如智能监控、自动驾驶辅助系统或工业物联网实时分析中，对系统资源提出的独特和严苛要求。

\subsection{硬件与软件环境}

宿主机（Host）选用在边缘计算和物联网（$\text{IoT}$）网关领域广泛应用的高性能嵌入式平台 $\text{Raspberry Pi 4 Model B}$。其核心硬件配置为 $\text{Broadcom BCM2711 SoC}$，集成了四核 $\text{Cortex-A72}$ ($\text{ARM v8}$) $\text{64}$ 位处理器，主频锁定在 $\text{1.5GHz}$。内存方面，宿主机配备 4GB LPDDR4 SDRAM，但为了更真实地模拟资源高度受限的边缘场景，通过内核启动参数 $mem=2G$ 的方式，将宿主机可用内存严格限制在 $\text{2GB}$。存储介质采用 32GB Class 10 MicroSD 卡，该介质由于具备 $\text{NAND Flash}$ 典型的物理特性，如显著的读写非对称性和随机写入性能衰减，被特意用于验证第 $6$ 章所提出的面向 $\text{Flash}$ 的存储优化机制的有效性。在软件栈层面，宿主机运行稳定的 $\text{Ubuntu Server 20.04 LTS}$（内核版本 $\text{5.4.0}$），虚拟化层则基于主流的 $\text{QEMU 6.2.0}$ 配合 $\text{KVM}$ 实现硬件辅助虚拟化。客户机（Guest VM）被配置为 $\text{2 vCPU}$ 和 $\text{1GB}$ 内存，并运行极简的 $\text{Alpine Linux 3.18}$ 操作系统，以模拟典型的边缘场景。

\subsection{实验负载}

参考边缘场景下的典型应用 ，本研究选取了以下负载进行测试：

\begin{enumerate}
  \item \textbf{Idle（空闲）}：仅运行操作系统基础守护进程，用于测定系统底噪。
  \item \textbf{SQLite（轻量级数据库）}：执行频繁的插入与更新操作，产生大量随机 I/O，用于验证存储子系统的处理能力。
  \item \textbf{7zip（数据压缩）}：产生持续的高内存带宽占用，用于评估在高频脏页产生场景下系统的承载能力。
  \item \textbf{Memcached（缓存系统）}：执行高并发读写操作，模拟典型的内存密集型缓存访问场景。
  \item \textbf{OpenCV（计算机视觉处理）}：运行图像处理与特征提取任务，用于模拟中等强度的计算与内存访问混合型负载。
  \item \textbf{YOLO（实时目标检测）}：执行实时推理任务，具有高 CPU/GPU 占用与连续内存写入特征，用于测试快照操作对计算密集型 AI 工作负载的影响。
  \item \textbf{TinyLlama（轻量级大语言模型推理）}：运行小型 Transformer 模型的推理任务，用于模拟典型的内存访问密集与中等计算开销的 LLM 工作负载。
\end{enumerate}

\subsection{对比基准}

本研究选取以下三种快照机制作为对比基准：  

\begin{enumerate}

\item \textbf{QEMU}：基于 QEMU 6.2.0 提供的原生预拷贝（pre-copy）机制。
\item \textbf{MLLS}：采用 MLLM提出的内存写入工作集感知迁移方法的快照机制。
\item \textbf{FLIC-DRAM}：基于 FLIC的工作集识别方法进行优化的快照机制。

\end{enumerate}

\section{关键技术微基准测试}

本节通过隔离测试，分别验证 HPRO 架构中热点感知机制、动态快照策略和存储与I/O优化三个核心模块的性能贡献。

\subsection{热点感知机制的性能评估}

在本节中，我们旨在对 $\text{HPRO}$ 系统中集成的热点感知算法的性能进行量化评估，重点关注其检测准确性与系统开销两个核心指标。检测精度（$\text{Accuracy}$）被精确定义为 $\text{Accuracy} = \frac{\sum_{\text{detected}}}{\sum_{\text{real}}}$，其中 $\sum_{\text{detected}}$ 代表通过 $\text{HPRO}$ 算法识别出的热页数量，而 $\sum_{\text{real}}$ 则表示实际热页总数。确定实际热页的标准是通过每 $\text{30}$ 秒进行一次周期性采样，并在累计超过 $\text{60}$ 个样本的观察窗口内对页面的写入计数进行统计，随后依据这些计数进行降序排列来确定。同时，本研究还将 $\text{HPRO}$ 算法与传统的{\color{red}基于计数器}方法（表示为 $\text{COUNTER}$）进行了对比，该 $\text{COUNTER}$ 方法的机制是基于对最后 $\text{10}$ 个时间样本内的访问次数进行采样和计数，并根据此短期排序列表选择指定比例的页面作为热门页面，以此论证 $\text{HPRO}$ 算法在准确性与开销间的平衡优势。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/hello.png}
    \caption{SPI反馈式令牌桶流控}
    \label{fig:x}
\end{figure}

{\color{red}
如图 $4$ 所示的实验结果清晰地表明，本研究提出的热点感知技术在检测精度方面与传统的基于计数器方法（$\text{COUNTER}$）展现出高度的一致性。在多数工作负载场景中，两种方法的热页检测精度曲线几乎重合，即使在差异最显著的 $\text{7zip}$ 压缩任务中， $\text{HPRO}$ 算法与 $\text{COUNTER}$ 方法的检测精度差异也仅有 $2\%$，这充分证明了 $\text{HPRO}$ 算法在不牺牲准确性的前提下，成功实现了轻量化设计。然而，在时间开销方面，本方法展现出显著的优势。这种优势随着系统内存规模的扩大而愈发明显：当系统内存大小配置为 $\text{2GB}$ 时，$\text{HPRO}$ 方法的平均时间开销为 $\text{55ms}$，仅占 $\text{COUNTER}$ 方法开销的 $57.2\%$。更重要的是，随着内存配置升级到 $\text{32GB}$ 这一更具挑战性的规模时，传统的 $\text{COUNTER}$ 方法的时间开销急剧增加至 $\text{327ms}$，而 $\text{HPRO}$ 算法的时间开销则稳定地维持在 $\text{55ms}$ 的水平，相对于传统方法获得了 $\text{272ms}$ 的巨大优势。鉴于在边缘计算环境中，快照操作的平均停机时间通常仅为数百毫秒，这种 $272\text{ms}$ 的开销减少在保障服务连续性和用户体验方面具有重大的实际意义和价值。
}

\subsection{热页动态转换的适应性评估}

本节旨在评估 $\text{HPRO}$ 系统中热点感知算法对工作负载动态变化即热页转换的反应速度和适应能力。这种适应性是衡量算法在实际、多变边缘环境中的鲁棒性的关键指标。我们通过执行连续快照操作来模拟系统运行中的周期性状态保存，并在此过程中监测算法对热页集合动态更新的响应。实验中设计了一个具有代表性的工作负载切换场景：客户机的工作负载从内存密集型应用Memcached（通常表现为少量高频访问的热页）动态切换到 $\text{I/O}$ 密集型应用SQLite（可能导致热页分布更为分散或频繁变化）。评估的重点集中于在工作负载切换发生时，热点检测精度随时间变化的行为模式。快照（即热页检测周期）被设定为固定的 $\text{30}$ 秒间隔。为了进行全面且具有说服力的比较分析，我们引入了传统的基于计数器方法（即前文提到的 $\text{COUNTER}$）作为对比基准。通过对比 $\text{HPRO}$ 算法与 $\text{COUNTER}$ 方法在应对 Memcached 到 $\text{SQLite}$ 这种典型工作负载转换时，检测精度恢复到稳定高水平所需的时间和精度曲线的平稳性，来严格评估 $\text{HPRO}$ 算法在处理热页动态转换方面的有效性和优越性。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/hello.png}
    \caption{SPI反馈式令牌桶流控}
    \label{fig:x}
\end{figure}

{\color{red}
如图 $5$ 所示，实验结果清晰地描绘了本研究提出的 $\text{HPRO}$ 热点感知算法与传统基于计数器方法（$\text{COUNTER}$）在面对工作负载动态切换时，其热点检测精度随时间推移而发生的瞬态变化与恢复过程。在初始的 $\text{Memcached}$ 稳定负载阶段，两种算法均能保持在较高的准确性水平，体现了它们在静态负载环境下的有效性。然而，当系统经历从 $\text{Memcached}$ 到 $\text{FIO}$ 的工作负载转换并引发大规模热点页面集合的动态迁移时，两种算法的检测精度均不可避免地出现了显著的瞬时下降。关键的优势体现在随后的恢复阶段：本研究提出的 $\text{HPRO}$ 算法展现出卓越的自适应能力和响应速度，能够在** $5$ 个采样间隔内迅速地将检测精度恢复至可靠且稳定的高准确度水平；与之形成鲜明对比的是，传统的 $\text{COUNTER}$ 技术则需要更长的周期，耗费 $8$ 个采样间隔**才能达到与之相当的准确度状态。这一关键差异有力地证明了 $\text{HPRO}$ 算法在处理边缘计算环境中常见的瞬时高变化性工作负载时的优越性，确保了系统在动态调整过程中能够更快地恢复对关键资源的精确感知与高效管理。
}

\subsection{基于 SPI 的连续快照动态增量策略评估}

本节旨在对 $\text{HPRO}$ 系统提出的基于 $\text{SPI}$ 的连续快照动态增量策略进行系统性评估，核心关注点在于验证该策略的技术可行性及其所涉及的潜在性能权衡。为在多样化的边缘场景中进行测试，实验设计了三组具有代表性的工作负载场景，并对每组场景执行间隔为两分钟的连续快照操作，这些场景包括：系统处于低负载的基础空闲状态、典型的边缘 $\text{AI}$ 推理任务$\text{TinyLlama}$场景，以及资源高度竞争的$\text{CPU}$ 旋转与 $\text{TinyLlama}$ 组合混合负载场景。为量化该策略的有效性，我们定义了两个关键指标：可用性（$\text{Availability}$）和性能损失（$\text{Performance Overhead}$）。其中，可用性被精确定义为 $\log_{10} \left( 100 \cdot \frac{N_{\text{flush}}}{N_{\text{dirty}}} \right)$，用于衡量系统在快照间隔期间数据刷新效率，其中 $N_{\text{flush}}$ 是刷新到磁盘的脏数据次数，$N_{\text{dirty}}$ 是同期产生的脏页面总数；该指标的最大值 $\text{2}$ 对应于写时复制（$\text{COW}$）机制，而低于零的值则表示系统主动保留了绝大部分脏数据以最小化停机时间。性能损失则被定义为额外时间成本与标准执行时间成本之比，用于量化快照操作对正在运行的业务工作负载所带来的执行效率影响，从而全面评估该动态增量策略的综合性能。

{\color{red}
图 $7\text{a}$ 详细展示了在不同工作负载场景下，系统在 $\text{30}$ 秒固定间隔内将脏数据刷新到磁盘所达到的**可用性**指标（$\text{Availability}$）。分析结果表明，在**空闲工作负载**（$\text{Idle}$）条件下，系统的可用性维持在最高的数值 $\text{2}$，根据前文的定义，这反映了系统执行了高频率的脏数据刷新操作，行为模式类似于标准的写时复制（$\text{COW}$）机制。然而，在 **$\text{7zip}$ 工作负载**场景下，由于虚拟机（$\text{VM}$）内部存在显著的空闲时间或低活跃度周期，这促使我们提出的动态增量策略将可用性降至零以下，表明系统判断此时更适合采用**写时复制（$\text{COW}$）方法**。在 $\text{COW}$ 模式下，每一次写入操作都会被立即捕获和保存，从而有效确保了快照操作的高性能和低延迟。与此形成对比的是，当采用另一种工作负载时（上下文暗示的另一种高负荷或高脏率场景，但此处根据 $\text{7zip}$ 行为的描述，应是描述该策略根据 $\text{7zip}$ 低活跃度特性，选择切换到**惰性增量（Lazy Incremental）方法**），该方法会在连续快照间隔之间跳过或最小化脏数据的刷新。进一步分析**混合工作负载**场景可以观察到，系统的脏数据刷新次数会随着 $\text{7zip}$ 负载比例的增加而逐渐减少。具体量化结果显示，当 $\text{7zip}$ 工作负载的比例从 $\text{5\%}$ 上升到 $\text{50\%}$ 时，脏数据的刷新比率（$\frac{N_{\text{flush}}}{N_{\text{dirty}}}$）从 $\text{1.35}$ 显著下降至 $\text{0.23}$，同时，平均保存粒度也随之从 $\text{4.5}$ 页扩大到 $\text{59}$ 页。这些数据有力地证明了 $\text{HPRO}$ 的动态增量策略能够根据工作负载的实时特性智能地调整数据刷新频率和粒度，从而优化存储资源的使用和快照性能。
}


