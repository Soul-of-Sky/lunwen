%!TEX root = ../main.tex

\chapter{相关技术概述}
\label{section2}

\section{轻量级虚拟化架构}

在边缘计算与物联网设备快速普及的背景下，虚拟化技术正从传统数据中心逐步向资源受限的终端节点延伸。与云端服务器相比，边缘设备普遍具有处理器性能有限、可用内存容量较小、存储介质受限（如 eMMC、UFS 或低功耗 SSD）等特点。因此，其虚拟化架构不仅需要提供可靠的资源隔离和安全保障，还必须在系统开销、启动速度、内存占用和外设支持等方面做出更严格的优化。基于这些需求，轻量级虚拟化逐渐成为边缘智能基础设施的核心技术路线，能够在不牺牲隔离性和系统功能的前提下，为设备提供灵活的多实例运行环境。

本节首先对当前主流的轻量级虚拟化技术进行梳理，分析其在边缘场景下的适配性；随后进一步介绍 CPU、内存和 I/O 虚拟化机制，作为后续快照性能分析的重要基础。

\subsection{主流轻量级虚拟化技术}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/qemu.png}
    \caption{QEMU/KVM 架构}
    \label{fig:qemu}
\end{figure}

面向资源受限环境的虚拟化技术大体呈现“通用型方案”与“专用型方案”并行发展的格局。不同方案体现了对启动速度、兼容性、隔离力度和资源占用之间的不同权衡。

\textbf{（1）QEMU/KVM: 轻量级虚拟化中的通用方案}

QEMU/KVM是当前虚拟化技术领域中生态系统最为成熟、设备兼容性最为完备的技术组合，已被广泛应用于服务器虚拟化及高级边缘设备中。鉴于本研究的实验平台亦基于此体系构建，有必要对其结构与技术特点进行系统性的阐述。

图 \ref{fig:qemu} 展示了 QEMU/KVM 的混合架构，该架构清晰界定了用户态设备模拟与内核态 vCPU 执行的边界：客户机指令在硬件上直接执行，而涉及外设访问的 I/O 操作则需经历从内核态陷入并转发至用户态的多级路径，这种频繁的模式切换构成了整体虚拟化开销的主要来源。在这一基础上，QEMU/KVM 的运行形态可被视为一种典型的分层式设计，其完整行为可精确描述为“内核层虚拟机监控程序（Hypervisor）与用户态设备模拟”的协同工作模式：

\begin{itemize}

\item \textbf{KVM（Kernel-based Virtual Machine）\cite{kivity2007kvm}}：它以内核模块的形式位于Linux内核空间，充当Type-1 Hypervisor的角色。KVM的核心功能在于处理与硬件执行路径密切相关的任务，包括CPU虚拟化、内存地址翻译（MMU）以及异常陷入处理。通过充分利用硬件辅助虚拟化扩展（如Intel VT-x、AMD-V或ARM VE），绝大多数客户机（Guest）指令得以在硬件上直接执行，从而显著降低了传统全软件虚拟化的指令陷入与上下文切换开销。

\item \textbf{QEMU\cite{bellard2005qemu}}：作为用户态进程运行，QEMU主要承担外围设备的模拟、系统管理与控制平面操作。这涵盖了虚拟网络、虚拟块设备、PCI总线模型、图形输出、以及BIOS/UEFI启动等关键功能模块。得益于其丰富的功能，QEMU具备高度的灵活性，可通过编译裁剪等方式，有效地适配资源受限边缘计算平台对性能和资源限制的需求。

\end{itemize}

在边缘计算场景中，QEMU/KVM方案展现出显著的优势：首先，其具备运行标准Linux发行版或轻量级Windows系统的能力，保障了良好的软件生态兼容性；其次，通过配合Virtio半虚拟化驱动，能够有效降低I/O模拟带来的性能开销；更重要的是，它支持设备透传（Passthrough）功能，允许直接访问USB设备或专用加速器（如GPU/FPGA），这对于在边缘侧部署人工智能推理等对硬件加速有强依赖的业务，提供了关键的支撑能力。尽管QEMU/KVM的功能完整性使得其资源占用略高于某些专用型、极简化的虚拟化方案，但在需要运行复杂软件栈或依赖于成熟生态系统的边缘计算部署中，该方案仍具有不可替代的显著优势。

\textbf{（2）Firecracker：面向极致轻量化的微虚拟机}

Firecracker 是由亚马逊网络服务（AWS）针对无服务器计算（Serverless）场景专门设计和优化的轻量级虚拟机监控器\cite{agache2020firecracker}。其核心设计目标在于在确保安全隔离的前提下，实现高密度部署和极低资源消耗，以高效支持 AWS Lambda 与 AWS Fargate 等关键服务\cite{agache2020firecracker,aws_firecracker_blog}。

相较于 QEMU/KVM 等传统 Hypervisor，Firecracker 采取了极简主义的架构策略，旨在最大程度地减小攻击面和系统开销\cite{kivity2007kvm}。具体而言，它彻底精简了虚拟硬件模型，主动剔除了 PCI 总线、图形设备、传统 BIOS 以及大量遗留外设的支持，仅保留了对虚拟机运行至关重要的 Virtio-net（网络）和 Virtio-blk（块存储）设备\cite{agache2020firecracker}。此外，其核心组件采用具有内存安全性保证的 Rust 语言重写，显著降低了代码的复杂性并提升了整体的内存安全性\cite{agache2020firecracker}。

这种极简化的设计为 Firecracker 实现毫秒级启动时间和快速快照恢复奠定了基础\cite{agache2020firecracker}。Firecracker 所推行的微虚拟机（MicroVM）概念，尤其在快照恢复机制上，与本研究关注的快照性能优化领域存在直接的关联性。它创新性地摈弃了传统 Hypervisor 依赖全量内存读取的快照恢复模式。相反，Firecracker 通过内存映射机制，在快照文件与虚拟机运行状态之间建立惰性映射，从而实现内存页的按需、逐页加载\cite{firecracker_snapshot}。这种避免全量 I/O 操作的延迟加载策略，极大地缩短了恢复时间，为在资源受限环境中实现高效、快速的快照恢复提供了重要的方法论基础和工程实践启示。

\textbf{（3）gVisor 与 Kata Containers：沙箱化及安全容器}

为解决传统容器技术在多租户环境和边缘计算中面临的内核共享安全风险，业界发展出了以 gVisor 和 Kata Containers 为代表的沙箱化和安全容器方案，旨在提供接近虚拟机的隔离能力\cite{pahl2015containers}。

gVisor 是由 Google 推出的一种独特的进程级虚拟化方案\cite{gvisor_usenix}。其核心机制是通过一个在用户态运行的内核来拦截并代理所有来自客户应用程序的系统调用。这种设计将应用进程与宿主机内核完全解耦，提供了一种强大的安全沙箱环境，有效地限制了潜在的攻击面\cite{gvisor_usenix}。然而，这种隔离的实现并非没有代价。由于每一个系统调用都需要经过用户态内核的拦截、翻译与代理过程，gVisor 不可避免地引入了较高的运行时开销，影响了应用的执行效率\cite{felter2015containers}。此外，gVisor 仅专注于进程隔离，不支持底层硬件（例如 GPU 或专用加速器）的直接透传，这一局限性显著限制了其在需要硬件加速的边缘人工智能等高性能计算场景中的应用价值\cite{gvisor_limitation}。

与 gVisor 的进程级隔离不同，Kata Containers 采取了一种融合型的方案，旨在结合标准容器的启动速度与传统虚拟机的强大隔离性\cite{katacontainers_paper}。Kata Containers 遵循开放容器倡议（Open Container Initiative, OCI）标准，但其底层实现将每个容器包裹在一个独立的轻量级虚拟机内\cite{katacontainers_paper}。因此，Kata Containers 本质上仍是一个安全容器运行时，其硬件隔离能力依赖于底层的 Hypervisor。它能够灵活地选择不同的 Hypervisor 作为后端，例如 QEMU 以实现广泛的兼容性\cite{kivity2007kvm}，或选择 Firecracker 以追求极致的启动速度和最小的资源占用\cite{agache2020firecracker}，从而在不同部署场景中平衡安全性和性能需求。

\subsection{核心虚拟化机制}

在虚拟化环境中，Hypervisor捕获并持久化虚拟机的完整运行状态是快照操作的核心。因此，系统性地理解底层CPU、内存以及I/O虚拟化机制的运作方式，是分析快照操作开销来源和确定优化空间的关键前提。

\textbf{（1）CPU 虚拟化}

现代嵌入式处理器（如 ARM Cortex-A 系列）普遍集成了硬件辅助虚拟化扩展，使 Guest 指令能够在接近原生性能的条件下执行。典型架构采用分层特权模式，宿主机运行于高权限的 EL2，而 Guest 操作系统运行于 EL1。当 Guest 访问特权寄存器、页表或设备等敏感资源时，硬件触发 VM Exit，将控制权移交给 Hypervisor，该过程带来显著的上下文切换和流水线清空开销\cite{popek1974formal}。

在快照机制中，脏页追踪通常通过页表写保护实现。Guest 写入受保护页面时会触发异常并引发 VM Exit。对于主频较低、核数受限的边缘设备而言，高频 VM Exit 会严重侵占 CPU 时间片，显著影响前台任务的实时性和稳定性\cite{wang2014vm}。

\textbf{（2）内存虚拟化}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/stage2.png}
    \caption{二阶段地址翻译流程}
    \label{fig:stage2}
\end{figure}

现代处理器通过扩展页表（EPT）与 Stage-2 页表机制，在硬件层面实现了从客户机物理地址（GPA）到宿主机物理地址（HPA）的二维地址转换，大幅降低了传统影子页表的维护开销\cite{uhlig2005intel}。该机制为访问位与脏位的硬件自动维护提供了基础支撑。

在此基础上，传统脏页检测仍主要依赖写保护异常机制，但该路径在高并发负载下开销显著。为此，本研究通过引入“热点感知”机制，利用硬件维护的访问位与脏位进行低频轮询，以减少异常触发次数，从而降低快照过程对 Guest 执行流的干扰\cite{kivity2007kvm}。

\textbf{（3）I/O 虚拟化}

在边缘环境中，Virtio 半虚拟化机制因其低延迟和高吞吐特性被广泛采用。该模型通过共享内存队列（Virtqueue）实现 Guest 与 Host 设备模型之间的异步通信，有效避免了传统设备完全仿真的高额开销\cite{russell2008virtio}。

然而，快照操作本身高度依赖存储 I/O 带宽。大量内存数据的连续写入可能占满宿主机 I/O 资源，引发前台业务的严重时延抖动和队头阻塞问题。因此，对快照 I/O 流量实施精细化流控和优先级调度，是确保边缘业务稳定性的关键\cite{matthews2007xenstore}。

\section{虚拟机快照机制原理}

虚拟机快照技术旨在通过持久化保存虚拟机在特定时刻的完整运行状态，实现系统的高可用性保障、故障恢复及状态回滚。一个完整的虚拟机状态不仅包含静态的磁盘数据，更核心的是其易失性的运行时状态，主要包括物理内存数据、CPU 寄存器上下文以及外设设备状态。根据内存数据的保存时机与虚拟机执行流的停顿策略，现有的快照机制在学术界与工业界演化出了停止并拷贝（Stop-and-Copy）、预拷贝（Pre-copy）及后拷贝（Post-copy）三种经典范式。这些范式在停机时间、总快照耗时以及对前台业务的性能干扰这三个关键指标上呈现出截然不同的权衡特性，且在面对资源受限环境的异构物理约束时表现出显著的适应性差异。

\subsection{停止并拷贝}

停止并拷贝是快照技术发展初期最基础、实现逻辑最为直观的机制，其核心思想是“以停机换一致性”\cite{dunlap2002revirt}。该机制的执行流程遵循严格的线性序列：首先，Hypervisor 发送指令强制挂起虚拟机的所有 vCPU，屏蔽所有外部中断并暂停 I/O 操作，确保系统状态处于静止状态；随后，快照引擎遍历虚拟机的全量物理内存空间，将其作为连续的二进制流顺序写入持久化存储介质，同时对设备寄存器状态进行序列化保存；待所有数据落盘并完成元数据提交后，虚拟机才被允许恢复执行。停止并拷贝的最大优势在于其实现复杂度低且能够提供最严格的数据强一致性保障\cite{ta2008virtual}。由于快照期间虚拟机完全停止，不存在内存状态变更导致的“脏数据”问题，因此无需复杂的脏页追踪或一致性校验机制。然而，停止并拷贝的致命缺陷在于其停机时间与虚拟机内存大小呈严格的线性正比关系。在资源充裕的数据中心，得益于高带宽的 NVMe SSD 阵列，这一延迟尚可被容忍；但在资源受限的边缘计算环境中，该缺陷被显著放大\cite{zhong2016flic,heiser2008role}。嵌入式边缘设备通常采用 NAND Flash 作为主存储，其持续写入带宽受限于控制器性能和 Flash 物理特性，仅维持在数十 MB/s 的水平。这意味着，保存一个 1GB 内存规格的轻量级虚拟机，可能导致数十秒甚至更长时间的服务“黑洞期”。对于部署在边缘侧的工业控制系统、自动驾驶决策模块或实时视频分析应用而言，这种长时中断会导致控制指令丢失、传感器数据积压甚至引发严重的安全事故。因此，停止并拷贝虽然保证了数据安全，却牺牲了服务的连续性，难以满足边缘智能系统的高可用性需求\cite{shi2019memory}。


\subsection{预拷贝}

为了克服停止并拷贝停机时间过长的问题，预拷贝机制引入了并行保存与迭代逼近的思想，广泛应用于虚拟机的实时迁移与在线备份场景\cite{clark2005live}。该机制将内存保存过程重构为三个阶段：初始化阶段、迭代预拷贝阶段和停机拷贝阶段。在初始化阶段，Hypervisor 开启全局脏页追踪功能，通常利用硬件页表（如 Intel EPT 或 ARM Stage-2 Page Tables）的写保护机制来捕获内存写入操作\cite{uhlig2005intel}。在第一轮迭代中，虚拟机继续运行，系统在后台将全量内存页异步保存至目标存储；在随后的每一轮迭代中，系统仅保存上一轮保存期间被虚拟机修改过的“脏页”。该过程循环进行，旨在通过不断的增量保存，使得每轮产生的脏页量逐渐减少，直至剩余脏页量收敛至一个极小的阈值（例如几 MB）或达到预设的最大迭代次数。此时，系统进入短暂的停机阶段，保存剩余的极少量脏页及 CPU 状态，从而实现极短的停机切换\cite{hines2009post}。

理论上，预拷贝能显著压缩停机窗口，但其有效性建立在“内存保存速率显著大于脏页产生速率”这一关键假设之上。然而，在资源受限环境与 AI 负载的双重压力下，这一假设往往失效，导致严重的“不收敛”问题。一方面，边缘 AI 推理任务（如 CNN 卷积运算）和数据库应用属于典型的写密集型负载，其在运行时会产生大量中间张量或日志数据，导致内存脏页产生速率极高\cite{gobieski2019intelligence}。另一方面，边缘设备的网络带宽或 Flash 写入带宽极其有限，导致保存速率远低于脏页产生速率。这种“产出快、消化慢”的剪刀差现象，使得系统在每一轮迭代中积累的脏页量不仅没有减少，反而可能增加。为了防止快照过程无限期拖延，预拷贝算法通常会触发强制停机策略，最终退化为停止并拷贝。更糟糕的是，无效的迭代过程不仅未能缩短停机时间，反而因长时间开启脏页追踪，触发了海量的页表写保护异常（Page Faults/EPT Violations），导致 CPU 在宿主机与虚拟机模式间频繁切换，并消耗了宝贵的 I/O 总线资源，造成前台业务性能的严重下降\cite{shi2022memory}。


\subsection{后拷贝}

针对预拷贝在写密集负载下的收敛难题，后拷贝机制采用了“先停机，后保存”的逆向策略，旨在无论负载如何均能实现确定性的低延迟启动\cite{hines2009post}。在快照触发瞬间，系统执行一次极短的停机，仅暂停虚拟机保存 CPU 寄存器上下文、设备状态以及极少量的非分页内存（通常仅数 MB），随后立即在目标端或恢复点激活虚拟机运行。此时，虚拟机的绝大部分内存数据并未实际到位，而是通过用户态缺页处理机制（如 Linux 的 userfaultfd 或 KVM 的 async page fault）进行写时复制（Copy-On-Write, COW）\cite{wang2014vm}。当虚拟机尝试修改尚未恢复的内存页时，硬件会触发缺页异常，Hypervisor 拦截该异常并挂起对应的 vCPU，同时向源端或磁盘发起数据拉取请求；在数据就绪前，vCPU 保持阻塞状态。为了优化用户体验，后拷贝通常还会结合主动推送策略，在后台利用空闲带宽预先保存剩余页面，以减少未来的缺页概率。

虽后拷贝机制成功将快照的可见停机时间压缩至毫秒级，实现了“即时启动”，但它本质上是将保存开销从停机阶段平摊到了后续的运行过程中，这在资源受限环境下引发了新的性能危机\cite{shi2022memory}。嵌入式处理器的单核计算能力较弱，每一次缺页异常的处理流程都极为昂贵：它涉及内核态与用户态的上下文切换、Hypervisor 的逻辑判断、中断处理以及高延迟的磁盘 I/O 读取。在系统恢复初期，若业务进程需要修改大量的工作集内存，这将导致密集的缺页请求爆发，形成“缺页风暴”。此时，vCPU 大部分时间处于等待 I/O 完成的阻塞状态（I/O Wait），导致虚拟机性能出现剧烈抖动。对于 AI 推理任务，这种不确定的延迟抖动会导致推理结果无法在截止时间内返回；对于依赖看门狗（Watchdog）的工业控制系统，长时间的主线程阻塞甚至可能误触发系统复位。此外，后拷贝机制在快照完成前，系统处于“分裂状态”（状态一部分在内存，一部分在磁盘），若此时发生断电或网络中断，将导致不可恢复的数据损坏，这对边缘设备的可靠性构成了严峻挑战\cite{li2014consnap}。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/3copy.png}
    \caption{快照机制时序对比}
    \label{fig:3copy}
\end{figure}

为了直观比较上述三种机制的性能特征，图\ref{fig:3copy}展示了它们在时间轴上的执行差异。可以看到，停止并拷贝机制导致了与内存大小成正比的线性停机窗口；预拷贝通过迭代保存缩短了停机时间，但延长了总耗时；而后拷贝虽然实现了秒级启动，却将大量缺页开销推迟到了恢复后的运行阶段，导致性能曲线出现剧烈抖动。

表 \ref{tab:snapshot_metrics} 从停机时间、总耗时及运行时性能干扰三个核心维度，对上述三种经典机制进行了对比总结。可以看出，三种范式存在显著的性能“剪刀差”：停止并拷贝虽无运行时干扰，但停机延迟过高；预拷贝通过延长总耗时来换取短停机，但在高负载下效果存疑且拖累前台业务；后拷贝虽实现了即时启动，却将开销转移至运行阶段，引发严重的性能抖动。这种顾此失彼的特性表明，现有机制难以同时满足资源受限环境下低延迟、低干扰的高可用性需求，亟需引入基于热点感知的优化策略。

\begin{table}[htbp]
    \centering
    \caption{三种经典虚拟机快照机制的性能指标对比}
    \label{tab:snapshot_metrics}
    \renewcommand{\arraystretch}{1.2} % 调整行间距
    \setlength{\tabcolsep}{4pt} % 调整列间距以适应页面
    \begin{tabular}{l l l l l}
        \toprule
        \textbf{机制名称} & \textbf{核心策略} & \textbf{停机时间} & \textbf{总快照耗时} & \textbf{运行时性能损失} \\
        \midrule
        \textbf{停止并拷贝} & 
        全量停机保存 & 
        高 (线性增长) & 
        中 (单次保存) & 
        无 (恢复后全速运行) \\
        
        \midrule
        \textbf{预拷贝} & 
        迭代增量保存 & 
        依赖收敛性 (不稳定) & 
        \textbf{高} (多次迭代) & 
        中 (脏页追踪开销) \\
        
        \midrule
        \textbf{后拷贝} & 
        按需缺页拉取 & 
        \textbf{极低} (常数级) & 
        低 (分摊至运行期) & 
        \textbf{高} (严重缺页抖动) \\
        \bottomrule
    \end{tabular}
\end{table}

\section{内存脏页追踪技术}

无论是预拷贝机制中的迭代保存，还是后拷贝机制中的按需恢复，准确、高效地识别虚拟机运行期间修改的内存页面（即“脏页”）均是实现增量快照与状态一致性的核心前提。脏页追踪技术的性能直接决定了快照过程中的 CPU 开销、系统吞吐量以及最终的停机时间收敛性。随着虚拟化技术的发展，脏页追踪机制经历了从纯软件介入的页表保护向硬件辅助自动化记录的演进，旨在降低Hypervisor对前台业务的干扰。

\subsection{基于写保护的软件追踪机制}

传统的脏页追踪技术（如 KVM 的 Log-Dirty 模式）主要依赖于处理器内存管理单元（MMU）的写保护机制来实现\cite{wang2014vm}。其核心工作流程是：Hypervisor 首先遍历虚拟机的所有扩展页表项（如 Intel EPT 或 ARM Stage-2 PTE），将所有客户机物理内存页（GPA）的访问权限设置为“只读（Read-Only）”。当虚拟机内部的业务进程尝试对这些页面进行写入操作时，硬件会触发扩展页表违例（EPT Violation）或缺页异常（Page Fault），导致 CPU 强制暂停当前 vCPU 的执行并触发 VM Exit，使控制权从 Guest 模式陷入 Host 模式的 KVM 模块\cite{uhlig2005intel}。KVM 在异常处理程序中捕获故障地址，将对应的位图置位，随后解除该页面的写保护并恢复 vCPU 运行。虽然这种机制能够实现对每一次写入行为的精确捕获，但在资源受限的嵌入式边缘环境中，其性能开销被认为是灾难性的。每一次 VM Exit 都必然伴随着昂贵的上下文切换，涉及通用寄存器保存、TLB 刷新以及对 L1/L2 缓存的潜在污染。对于写密集型负载（如 AI 训练、大规模数据预处理等），内存写入行为极其频繁，将引发“中断风暴”效应，使得 vCPU 将绝大部分的执行时间消耗在陷出处理而非核心业务计算上，从而导致服务性能显著下降\cite{wu2016hot}。此外，在 QEMU/KVM 分层架构下，用户态的 QEMU 进程需要通过 ioctl 系统调用频繁地从内核态同步最新的脏页位图。这种跨态数据拷贝本身消耗了大量的 CPU 时间，且在 I/O 总线带宽受限的边缘设备上，进一步加剧了系统延迟和总线竞争问题，严重限制了快照机制在追求低延迟和高效率环境中的适用性\cite{kivity2007kvm}。


\subsection{硬件辅助追踪}

为了解决软件追踪带来的高昂开销，现代处理器架构引入了硬件辅助的脏页记录特性，旨在减少 VM Exit 的频率或完全消除软件干预\cite{wu2016hot}。Intel 推出的页修改日志（Page Modification Logging, PML）技术代表了“日志化批处理”的优化思路\cite{intel_pml_arxiv}。PML 允许 CPU 在硬件层面上，直接将发生写入的脏页物理地址记录到预分配在内存中的日志缓冲区（PML Buffer）中，整个过程无需触发页表异常。仅当该缓冲区填满（通常为 512 个条目）时，CPU 才会触发一次 VM Exit，通知 Hypervisor 进行批量日志处理。这种机制将原本“每次写入触发一次中断”的高频开销转化为“每 N 次写入（N=512）触发一次中断”的低频开销，显著降低了虚拟化层的 CPU 占用率。相关研究表明，PML 技术能够将迁移过程对用户应用的干扰降低至 1\% 以内，极大地提升了快照或热迁移操作的平滑性\cite{xu2019pmlperf}。

另一种更为轻量级和通用的路径是利用页表项中内嵌的硬件访问位（Access Bit）与脏位（Dirty Bit）。在 ARMv8.1 及现代 x86 架构中，处理器提供了硬件特性（如 FEAT\_HA\-FDBS），支持 MMU 在页面被读或写时自动将 Access/Dirty 位置 1，这一过程是原子性的，无需软件介入且不产生 VM Exit\cite{armv8manual_hafdbs}。Hypervisor 仅需周期性地扫描扩展页表即可获取内存的访问状态，并将已标记的位清零。虽然这种轮询方式的实时性略低于 PML 的事件驱动模式，但它彻底消除了写入中断，避免了 VM Exit 带来的所有负面效应（上下文切换、缓存污染）。对于计算资源极度匮乏且对性能抖动极度敏感的资源受限边缘计算平台而言，基于 Access/Dirty Bit 的异步追踪机制是构建低开销、非侵入式热点感知模型的理想基础\cite{ozawa2021spp}。


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/pml.png}
    \caption{软件写保护与硬件辅助追踪对比}
    \label{fig:pml}
\end{figure}

为了直观展示两种机制的开销差异，图\ref{fig:pml}对比了传统写保护与硬件辅助追踪的处理路径。可以看到，传统机制在每次写入时都会触发昂贵的 VM Exit 与上下文切换，形成‘中断风暴’；而硬件辅助机制则将记录工作下沉至处理器流水线，仅产生极低的日志记录开销，这正是解决边缘计算资源瓶颈的关键。

\subsection{细粒度与高并发追踪优化}

除了降低追踪开销，解决“伪共享”与“锁竞争”问题也是脏页追踪优化的重要方向\cite{barroso2018datacenter}。传统的脏页追踪粒度通常与标准页大小一致，即 4KB。这意味着即使页面中仅有一个字节被修改，整个 4KB 数据也必须被标记为脏页并被保存。在网络或存储带宽受限的边缘环境中，这造成了严重的 I/O 浪费，被称为写放大效应（Write Amplification, WA）\cite{hu2020writeamp}。Intel 提出的子页保护（Sub-Page Protection, SPP）技术通过将写保护粒度细化至 128 字节，允许 Hypervisor 更精确地定位和剔除无效数据\cite{intel_spp_isca}。这种细粒度追踪能够显著降低保存的数据量，提高网络资源的有效利用率。

在多 vCPU 高并发场景下，KVM 传统的全局脏页位图成为了一个严重的同步瓶颈，多个 vCPU 在更新位图时会引发激烈的锁竞争\cite{abel2022kvm}。为了解决这一问题，Linux 内核引入了 KVM Dirty Ring 机制\cite{linux_kvm_dirtyring}。该机制不再使用单一的全局锁，而是为每个 vCPU 维护一个独立的环形缓冲区（Ring Buffer）用于记录其产生的脏页信息。这有效地解耦了脏页收集过程中的同步锁，允许多个 vCPU 并行地执行写入和脏页记录操作，极大提升了大规模并发快照时的追踪效率与系统的可扩展性\cite{abel2022kvm}.


总的来说，脏页追踪技术的演进集中体现了虚拟化技术在隔离性与效率之间寻求最优平衡的努力。利用硬件辅助特性实现低开销、非侵入式的追踪，结合细粒度保护和高并发数据结构的优化，是解决资源受限环境下快照性能瓶颈的关键技术路径。

\section{嵌入式 Flash 存储特性}

与数据中心服务器普遍配备的高性能NVMe SSD阵列或机械硬盘（HDD）不同，资源受限的边缘计算节点主要采用NAND Flash介质作为主存储设备，包括板载eMMC、SD卡或嵌入式SSD。Flash存储凭借其固有的低功耗、抗震动、轻量化及小体积等优势，主导了嵌入式和边缘计算领域。然而，其独特的物理属性——特别是读写操作的非对称性与异地更新机制——给操作系统快照等涉及高强度、写密集型I/O的任务带来了严峻的性能挑战与可靠性隐患。

\subsection{读写非对称性与块擦除约束}

NAND Flash 的存储阵列在物理上被设计为严格的层次结构，数据存储的基本单位是页（Page，通常粒度为 4KB 至 16KB），而管理和维护的基本单位是包含多页的块（Block，通常大小为数百 KB 至数 MB）\cite{micron_nand_basics,yao2017flashsurvey}。这种层次结构决定了 Flash 操作粒度上的固有不对称性：读与写操作的最小粒度是页，而擦除操作的最小粒度却是大得多的块\cite{gal2018nand}。最为关键的物理约束是“写入前必须擦除”（Erase-before-Program）\cite{agrawal2008design}。由于 Flash 单元的物理特性，写入过程只能将比特位从 1 翻转为 0，而从 0 翻转回 1 这一重置操作，必须通过耗时且高电压的块擦除操作才能完成。这意味着，Flash 控制器无法像传统机械硬盘那样对旧数据进行原地覆盖\cite{yao2017flashsurvey}。当操作系统请求修改一个页面中的数据时，控制器不能直接写入，只能将新数据写到空白页，并将旧页标记为无效。由于块擦除操作具有极高的延迟（通常为毫秒级）\cite{micron_nand_latency}，这使得 Flash 介质在处理快照生成的随机小块写入时效率极低，并会显著阻塞 I/O 总线，导致系统 I/O 吞吐量的瞬时下降\cite{gal2018nand}.


\subsection{异地更新策略与闪存转换层（FTL）的资源消耗}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/flash.png}
    \caption{Flash 异地更新流程}
    \label{fig:flash}
\end{figure}

受限于上述严格的擦除约束，Flash存储天然不支持传统磁盘文件系统的原地更新模式。为了在不改变上层软件接口的前提下兼容Flash特性，引入了关键的中间件——闪存转换层（Flash Translation Layer, FTL）。FTL作为位于主控芯片中的固件或软件层，其核心功能在于维护一套动态的逻辑地址（LBA）到物理地址（PBA）的映射表，并执行异地更新（Out-of-Place Update）策略。具体而言，当主机请求修改一个逻辑页A时，FTL不会在原物理位置更新数据，而是将新的数据写入到一个全新的空白物理页 $P_{new}$，同时将原物理页 $P_{old}$ 标记为无效，并更新映射表以将LBA A指向 $P_{new}$。图\ref{fig:flash}详细描述了这一异地更新流程：由于物理介质不支持原地覆盖，逻辑地址的覆写操作迫使FTL不断重新映射物理页，导致旧数据所在的物理块中积累大量无效页，这正是后续垃圾回收与写放大效应产生的物理根源\cite{hwang2015f2fs, agrawal2008design}。这种机制虽然成功地透明化了底层的物理复杂性，但在虚拟机快照这类高负载场景下，操作系统产生的密集随机写入流将迅速导致物理空间碎片化。这种碎片化反过来迫使FTL必须维护一个庞大且动态变化的映射表，进而持续消耗宝贵的控制器内存与计算资源，使得FTL自身成为快照性能和能耗的关键瓶颈\cite{luo2018wisckey}。

\subsection{垃圾回收、写放大与设备磨损}

随着异地更新操作的持续进行，Flash存储介质中的无效页面会逐渐累积。当FTL检测到可用空闲块不足时，它必须触发垃圾回收（Garbage Collection, GC）机制来回收空间。GC过程是一个高开销的维护操作：FTL必须首先挑选包含最多无效页的目标块，然后将该块中所有仍然有效的页面迁移到一个全新的已擦除块中，最后对旧块执行耗时的块擦除操作以释放空间。GC过程不仅会给前台I/O引入不可预测的延迟尖峰，严重影响服务的实时性，更会引发核心问题——写放大效应。写放大是指实际写入到Flash介质的物理数据总量远大于主机逻辑写入的数据总量，通常以 $WA = \frac{Data_{\text{physical}}}{Data_{\text{logical}}} > 1$ 来度量。在GC过程中，为了迁移有效数据而产生的额外写入，使得WA因子急剧增加，加剧了存储I/O的负担，并迅速占用了系统I/O带宽。此外，Flash介质具有严格的耐久度限制，其寿命由编程/擦除周期（P/E Cycles）决定（如主流MLC/TLC颗粒仅有数千次的P/E Cycles）\cite{agrawal2008design,gal2012exploring}。传统的虚拟机快照技术若不加以优化，进行全量或高频增量落盘，将产生巨大的随机写入负载，这会剧烈触发GC与写放大，不仅导致系统性能迅速下降，更会加速Flash颗粒的物理磨损，迅速耗尽设备寿命。因此，设计面向Flash存储特性、能够实现I/O流的顺序化和精细流量控制的快照优化方案，是保障边缘计算系统性能稳定性和存储设备长期可靠性的必要条件\cite{zhong2016flic}.


\section{本章小结}

本章系统梳理了支撑本研究的核心技术背景，为后续的架构设计与优化策略奠定了坚实的理论基础。首先，深入阐述了面向边缘计算的轻量级虚拟化架构，指出了 KVM/QEMU 在资源受限环境下由上下文切换与 I/O 模拟带来的性能挑战；其次，详细剖析了停止并拷贝、预拷贝及后拷贝三种主流虚拟机快照机制的运行原理，揭示了它们在写密集型 AI 负载与弱 I/O 带宽双重约束下分别面临的停机延迟过高、迭代难以收敛及缺页风暴抖动等失效模式；再次，探讨了内存脏页追踪技术的演进路线，对比了传统软件写保护机制的高昂开销与现代硬件辅助特性的低损耗优势，明确了软硬协同感知的可行性；最后，分析了嵌入式 Flash 存储介质读写非对称、异地更新及写放大等物理特性。综上所述，现有的通用快照技术无法直接适配边缘计算环境的异构物理约束，构建一种能够感知内存热点、自适应系统资源并针对 Flash 特性优化的新型快照架构，是解决当前性能与可靠性瓶颈的必然选择。


