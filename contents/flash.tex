%!TEX root = ../main.tex

\chapter{面向 \text{Flash} 特性的存储与 \text{I/O} 优化}

在前两章中，我们通过基于双位老化算法的\emph{热点识别}（第 4 章）解决了脏页的精准捕获问题，通过基于 \text{SPI} 的\emph{动态调度策略}（第 5 章）解决了快照执行的时机与速率问题。然而，所有的快照策略最终都必须安全、高效地落实到物理存储介质的读写操作上，即“数据落地”环节。在边缘计算与物联网（\text{IoT}）场景中，出于成本、功耗和体积的考量，设备普遍采用 \text{NAND Flash} 作为主存储介质，常见形式包括板载 \text{eMMC}、\text{SD} 卡以及低功耗 \text{SSD}。

与数据中心服务器普遍配备的高性能 \text{NVMe SSD} 阵列不同，嵌入式 \text{Flash} 介质在物理特性上存在显著的局限性：其存储单元的\emph{读写不对称性}要求写入必须遵循“先擦除后写入”（Erase-before-Write）的物理约束，这直接导致了\emph{随机写性能黑洞}。具体而言，\text{Flash} 的最小写入单位是页（Page，通常 4\text{KB}-16\text{KB}），而最小擦除单位是块（Block，通常包含数百个页）。频繁的随机小块写入会导致严重的\emph{写放大（Write Amplification, WA）}效应，迫使闪存转换层（\text{FTL}）频繁执行垃圾回收（\text{GC}）操作，导致 \text{I/O} 延迟不可预测地飙升。此外，\text{Flash} 颗粒的编程/擦除（\text{P/E}）循环次数有限，不加优化的快照写入会加速介质磨损，严重缩短设备寿命。因此，如果快照系统不感知底层存储特性，直接将上层产生的零散脏页流刷入 \text{Flash}，必然会引发系统卡顿（Stall）并导致设备过早报废。本章将详细阐述 \text{HPRO} 系统在\emph{执行优化层（Execution Optimization Layer）}的底层机制，旨在从物理 \text{I/O} 层面打破资源受限环境的存储瓶颈。

\section{基于环形缓冲区的写合并机制（Write Coalescing）}

在快照过程中，尤其是采用写时复制（COW）策略处理温热页或在连续快照中捕获增量数据时，逻辑上会源源不断地产生大量 4\text{KB} 粒度的小块写入请求。对于 \text{Flash} 介质而言，这种“高频、离散、小粒度”的写入模式是性能杀手，它造成了 \text{FTL} 沉重且高延迟的负担。

\subsection{Flash 写放大与 \text{FTL} 映射压力分析}

当操作系统提交一个 4\text{KB} 的随机写入请求时，\text{Flash} 内部控制器无法直接覆盖旧数据（Out-of-Place Update）。\text{FTL} 必须将新数据写入一个新的物理页，并将旧物理页标记为无效。随着无效页面的累积，\text{FTL} 必须启动高成本的垃圾回收（\text{GC}）：将目标块中所有剩余的有效数据迁移到新块，然后才能执行耗时巨大的旧块擦除操作。在这个过程中，为了写入 4\text{KB} 的用户数据，控制器可能需要迁移数 \text{MB} 的有效数据并执行高延迟的擦除操作。这种物理写入量远大于逻辑写入量的现象即为写放大。理论分析表明，\text{WA} 系数与无效页面的分布和 \text{GC} 效率直接相关，随机写入使得 \text{FTL} 无法进行有效的顺序性优化。此外，大量的随机写入还会导致 \text{FTL} 的逻辑-物理地址映射表（\text{L2P} Table）频繁更新，这要求控制器消耗有限的 \text{SRAM} 资源进行映射管理，进一步拖慢了整体 \text{I/O} 响应速度。

\subsection{内存环形缓冲区架构设计}

为了从根本上缓解上述问题，本研究在快照引擎与块设备驱动之间引入了一层中间件——写合并层。其核心数据结构是一个驻留在内核态的\emph{环形缓冲区（Ring Buffer）}，作为快照数据的“整形蓄水池”。该缓冲区的设计目标是实现数据的持续聚合和高效查找。该缓冲区并非简单的 \text{FIFO}（先进先出）队列，而是设计为具备\emph{地址索引（Address Indexing）}功能的混合结构，包含以下关键组件：\emph{数据存储区}，一段连续的物理内存，用于暂存待落盘的脏页数据；\emph{哈希索引表（Hash Index）}，以页面的物理地址（\text{PFN}）为键，以缓冲区内的槽位偏移量为值，这使得系统能够以 $O(1)$ 的时间复杂度查询某个物理页当前是否已存在于缓冲区中；以及\emph{脏页链表}，维护当前缓冲区内所有有效脏页的写入顺序。这种结构确保了缓冲区具备了“在内存中原地更新数据”的能力，成为实现高效写合并的前提。

\subsection{逻辑覆写与物理合并策略}

利用上述数据结构，\text{HPRO} 系统实现了对高频更新页面的\emph{“逻辑覆写”（Logical Overwrite）}，从而大幅减少了实际下发到磁盘的物理 \text{I/O} 次数。具体处理流程如下：

\textbf{极热页拦截与原地更新：} 在快照期间，某些极热页面可能会被业务进程反复修改，例如某页面 $P$ 在短时间内经历了 $V_1 \rightarrow V_2 \rightarrow V_3$ 三次状态变更。传统 \text{COW} 机制会依次触发三次 \text{I/O} 操作，分别写入 $V_1, V_2, V_3$。然而，在 \text{HPRO} 机制下，当 $V_1$ 到达缓冲区时，系统为其分配槽位并建立索引。当 $V_2$ 到达时，系统通过索引快速发现 $P$ 已存在，于是直接在内存中用 $V_2$ 覆盖 $V_1$ 的数据位置，无需进行任何物理 \text{I/O}。同理，$V_3$ 覆盖 $V_2$。最终，缓冲区中仅保留了该页面的最终状态 $V_3$。通过这种逻辑覆写，系统过滤掉了所有的中间态无效数据，将 $N$ 次逻辑写操作最终合并为单次物理 \text{I/O}。

\textbf{顺序化落盘（Sequential Flushing）：} 数据不会在缓冲区中无限驻留。落盘操作由两个条件触发：空间阈值（缓冲区使用率达到高水位，如 $80\%$）或时间阈值（由第 5 章动态增量策略控制的定时器超时）。触发落盘时，系统不再按照请求到达的随机顺序写入，而是根据脏页的物理地址（\text{LBA}）对缓冲区内的数据执行\emph{排序（Sorting）}。排序后的数据被聚合成一个或多个大尺寸（如 $64\text{KB} - 512\text{KB}$）的\emph{顺序写入请求（Sequential Write）}下发给 \text{Flash}。这种优化成功将对 \text{FTL} 极不友好的随机写转化为友好的顺序写，显著降低了写放大系数，延长了 \text{Flash} 的使用寿命。

\section{多层级冗余消除与压缩技术}

在嵌入式虚拟化场景中，存储空间 ($S_{\text{storage}}$) 同样是极其稀缺的资源。无论是单次快照产生的镜像文件，还是连续快照产生的增量链，都包含了大量的、可消除的数据冗余。为了降低存储占用并进一步减少写入量，本研究引入了\emph{多层级冗余消除技术}，这是对 $S_{\text{storage}}$ 约束的直接回应，结合全页去重与细粒度压缩，从逻辑层面削减 \text{I/O} 负载。

\subsection{基于指纹库的全页去重（Page-level Deduplication）}

在边缘设备上，往往运行着多个同构的轻量级虚拟机或容器，它们共享相同的操作系统内核、基础库文件（如 \text{libc.so}, \text{libpython.so}）以及应用代码段。这些只读数据在内存中存在多份副本，是主要的冗余来源。

\text{HPRO} 系统实现了一个基于 \text{MurmurHash3} 算法的\emph{全局指纹库（Global Fingerprint Table）}。选择 \text{MurmurHash3} 是因为它具有极高的计算效率和低碰撞率，确保了在嵌入式 \text{CPU} 上执行哈希计算不会成为新的 $C_{\text{total}}$ 瓶颈。在写入任何页面之前，系统首先计算该页面的 128 位哈希值。系统在全局指纹库（通常采用开放寻址法的哈希表实现以优化内存占用）中查询该指纹。

如果查询\emph{命中（Hit）}，说明该数据内容在之前的快照或其他虚拟机中已存储过。系统仅在快照元数据中记录一个\emph{引用指针（Reference）}，格式为 $\langle\text{vm\_id}, \text{snapshot\_id}, \text{pfn}\rangle$，而不执行实际的数据写入。如果指纹未命中，则将该页面写入存储，并将新的指纹及存储位置添加到库中。这种全页去重机制通过消除\emph{跨机冗余}和\emph{跨快照冗余}，在多实例部署场景下能够实现可观的存储空间节省。

\subsection{256B 细粒度提取（Fine-grained Extraction）}

全页去重只能处理完全一致的页面。然而，在实际运行中，许多脏页仅发生了微小的变化。例如，数据库更新一条记录时，可能只修改了 4\text{KB} 页面中的几十个字节，但传统机制不得不保存整个 4\text{KB} 页面。这种“大页微小修改”造成了严重的 \text{I/O} 浪费和存储资源浪费。

针对这一问题，本研究引入了\emph{细粒度提取机制}。我们将标准的 4\text{KB} 页面逻辑划分为 $16$ 个 256\text{B} 的子块（Sub-chunk）。在保存脏页时，系统不仅对比整页哈希，还会对这些子块的内容变化进行逐个比对。对于每个子块，系统检查其是否为全零，或者是否与上一版本的对应子块相同。

这种差异比对机制的核心在于实现紧凑存储与位图索引的精确协同。系统仅将发生实际变更的\emph{非零子块}写入增量快照文件。同时，未变更的子块通过元数据中的位图（Bitmap, $16$ bits）进行标记跳过，而全零子块则通过特殊标记位记录，完全不占用任何物理存储空间。通过这种机制，逻辑写入粒度被从 4\text{KB} 成功缩小到 256\text{B}，从根本上解决了稀疏写入负载下的 \text{I/O} 浪费问题。正是这种 256\text{B} 的细粒度控制，使得 \text{HPRO} 系统在处理如 \text{Redis} 的随机 \text{Key} 更新等负载时，能够显著提高存储效率并节省存储资源。

\section{基于 \text{SPI} 反馈的 \text{I/O} 流量整形}

在 \text{HPRO} 的差异化传输策略（第 5.4 节）中，大量的冷寂页被安排在后台进行异步传输。虽然这是低优先级的任务，但在缺乏流控的情况下，后台迁移线程（Migration Thread）极易在短时间内占满 \text{Flash} 有限的写入带宽（通常仅为 $20\text{MB/s} - 40\text{MB/s}$）。这种 \text{I/O} 资源的独占会导致前台业务的关键 \text{I/O} 请求被迫在 \text{I/O} 调度器队列中排队，引发严重的“队头阻塞（Head-of-Line Blocking）”，表现为业务响应延迟剧烈抖动。

为了保障前台业务的\emph{服务质量（QoS）}，本研究设计了基于\emph{令牌桶算法（Token Bucket）}的 \text{I/O} 节流阀，并将其与第 5 章的 \text{SPI}（系统压力指数）深度联动，实现了自适应的流量整形。

\subsection{令牌桶流控模型}

系统为后台快照线程设置了一个令牌桶结构，用于精确控制写入速率。该桶由三个参数定义：\emph{令牌生成速率} ($R_{\text{token}}$)、\emph{桶容量}（决定了最大允许的突发写入量）和\emph{令牌消耗率}。后台线程每写入一定量的数据，必须从桶中申请并消耗对应数量的令牌。如果桶中令牌不足，后台线程将被强制挂起（Sleep），进入等待状态，直到有新的令牌注入桶中。该机制充当了资源分配的门卫，将后台快照任务的瞬时速率限制在桶容量之下，并将其平均速率限制在 $R_{\text{token}}$ 之内，从而确保了快照 \text{I/O} 始终低于 $B_{\text{io}}$ 的安全阈值。线程的挂起和唤醒虽然涉及轻微的上下文切换开销，但在防止核心业务 \text{I/O} 发生拥塞方面，是成本效益最高的控制手段。

\subsection{\text{SPI} 驱动的动态速率调节}

传统的令牌桶采用固定的令牌生成速率，无法应对负载剧烈波动的边缘环境。本研究建立了令牌生成速率 $R_{\text{token}}$ 与系统压力指数 $\text{SPI}$ 之间的\emph{负相关反馈函数}，实现了对 \text{I/O} 速率的动态自适应调整：

$$
R_{\text{token}} = R_{\text{max}} \times (1 - \text{SPI})
$$

该公式的动态调节逻辑构成了基于 \text{SPI} 的\emph{闭环控制系统}，其控制目标是\emph{最小化快照对业务 \text{I/O} 队列的贡献}。当业务负载升高（$\text{SPI}$ 趋近 $1.0$）时，公式计算出的 $R_{\text{token}}$ 急剧下降甚至归零。此时，后台快照传输自动减速或完全暂停（Suspend），将几乎全部 \text{I/O} 带宽让渡给前台业务，确保关键任务不受干扰，严格遵守 $B_{\text{io}}$ 约束。反之，当业务空闲（$\text{SPI}$ 趋近 $0$）时，令牌生成速率恢复至最大值 $R_{\text{max}}$。后台线程利用闲置带宽全速写入，以最快速度完成快照收尾。这种基于 \text{SPI} 的反馈机制，使得快照 \text{I/O} 流量能\emph{自适应地贴合系统的闲置资源曲线}，从而在不引发队头阻塞的前提下，最大化快照效率。

\section{本章小结}

本章聚焦于嵌入式 \text{Flash} 存储介质的物理特性与性能瓶颈，构建了 \text{HPRO} 系统的底层执行优化层。首先，通过环形缓冲区\emph{写合并机制}，利用地址哈希索引实现了逻辑覆写与顺序化落盘，将上层策略产生的高频随机逻辑写转化为低频的顺序物理写，从源头上降低了 \text{IOPS} 需求并显著缓解了写放大效应。其次，利用\emph{全局指纹库去重}与 $256\text{B}$ \emph{细粒度压缩技术}，最大限度地消除了跨快照、跨虚拟机及页内的冗余数据，显著节省了宝贵的存储空间并减少了写入磨损。最后，设计了基于 \text{SPI} 反馈的\emph{令牌桶流控}，实现了后台快照流量对前台业务负载的自适应避让。这些底层优化措施与第 4 章的热点感知、第 5 章的动态调度共同构成了软硬协同的快照优化闭环，确保了上层的高效策略能够安全、稳定地落地于资源受限的物理硬件之上。至此，\text{HPRO} 系统的设计与实现论述完毕，下一章将通过实验数据验证其在真实环境下的性能表现。
