%!TEX root = ../main.tex

\chapter{面向 \text{Flash} 特性的存储与 \text{I/O} 优化}

在前两章中，我们通过基于自适应多位老化算法的热点识别解决了脏页的精准捕获问题，通过基于热度感知和资源反馈的动态快照策略解决了快照执行的时机与速率问题。然而，所有的快照策略最终都必须安全、高效地落实到物理存储介质的读写操作上，即“数据落地”环节。在边缘计算与物联网场景中，出于成本、功耗和体积的考量，设备普遍采用 \text{NAND Flash} 作为主存储介质，常见形式包括板载 \text{eMMC}、\text{SD} 卡以及低功耗 \text{SSD}。

与数据中心服务器普遍配备的高性能 \text{NVMe SSD} 阵列不同，嵌入式 \text{Flash} 介质在物理特性上存在显著的局限性：其存储单元的读写不对称性要求写入必须遵循“先擦除后写入”的物理约束，这直接导致了随机写性能黑洞\cite{yao2017flashsurvey, gal2012exploring, hu2020writeamp}。具体而言，\text{Flash} 的最小写入单位是页（Page，通常 4\text{KB}-16\text{KB}），而最小擦除单位是块（Block，通常包含数百个页）。频繁的随机小块写入会导致严重的写放大（Write Amplification, WA）效应，迫使闪存转换层（Flash Translation Layer, FTL）频繁执行垃圾回收（Garbage Collection, GC）操作，导致 \text{I/O} 延迟不可预测地飙升。此外，\text{Flash} 颗粒的编程/擦除（\text{P/E}）循环次数有限，不加优化的快照写入会加速介质磨损，严重缩短设备寿命\cite{hu2020writeamp, gal2018nand}。因此，如果快照系统不感知底层存储特性，直接将上层产生的零散脏页流刷入 \text{Flash}，必然会引发系统卡顿并导致设备过早报废。本章将详细阐述 \text{HPRO} 系统在执行优化层的底层机制，旨在从物理 \text{I/O} 层面打破资源受限环境的存储瓶颈。


\section{基于环形缓冲区的写合并机制}

在快照过程中，尤其是采用写时复制策略处理温热页或在连续快照中捕获增量数据时，逻辑上会源源不断地产生大量 4\text{KB} 粒度的小块写入请求。对于 \text{Flash} 介质而言，这种“高频、离散、小粒度”的写入模式是性能杀手，它造成了 \text{FTL} 沉重且高延迟的负担。

\subsection{Flash 写放大与 \text{FTL} 映射压力分析}

当操作系统提交一个 4\text{KB} 的随机写入请求时，\text{Flash} 内部控制器无法直接覆盖旧数据。\text{FTL} 必须将新数据写入一个新的物理页，并将旧物理页标记为无效\cite{yao2017flashsurvey, gal2012exploring}。随着无效页面的累积，\text{FTL} 必须启动高成本的GC：将目标块中所有剩余的有效数据迁移到新块，然后才能执行耗时巨大的旧块擦除操作。在这个过程中，为了写入 4\text{KB} 的用户数据，控制器可能需要迁移数 \text{MB} 的有效数据并执行高延迟的擦除操作\cite{hu2020writeamp}。这种物理写入量远大于逻辑写入量的现象即为写放大。理论分析表明，\text{WA} 系数与无效页面的分布和 \text{GC} 效率直接相关，随机写入使得 \text{FTL} 无法进行有效的顺序性优化。此外，大量的随机写入还会导致 \text{FTL} 的逻辑-物理地址映射表（\text{L2P} Table）频繁更新，这要求控制器消耗有限的 \text{SRAM} 资源进行映射管理，进一步拖慢了整体 \text{I/O} 响应速度。

\begin{table}[htbp]
    \centering
    \caption{嵌入式 Flash 存储介质的物理约束与快照挑战}
    \label{tab:flash_constraints}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l l l l}
        \toprule
        \textbf{物理特性} & \textbf{约束机制} & \textbf{快照场景痛点} & \textbf{性能后果} \\
        \midrule
        \textbf{读写不对称} & 
        \begin{tabular}[c]{@{}l@{}}按页写入 (4KB)\\按块擦除 (MB级)\end{tabular} & 
        \begin{tabular}[c]{@{}l@{}}随机小块写入\\(逻辑覆盖)\end{tabular} & 
        \begin{tabular}[c]{@{}l@{}}\textbf{写放大 (WA)}\\吞吐量剧降\end{tabular} \\
        \midrule
        \textbf{异地更新} & 
        不能原地覆盖 & 
        \begin{tabular}[c]{@{}l@{}}无效页累积\\触发 GC\end{tabular} & 
        \begin{tabular}[c]{@{}l@{}}\textbf{不可预测延迟}\\业务卡顿\end{tabular} \\
        \midrule
        \textbf{P/E 寿命} & 
        擦写次数有限 & 
        \begin{tabular}[c]{@{}l@{}}高频元数据\\日志刷新\end{tabular} & 
        \begin{tabular}[c]{@{}l@{}}\textbf{介质磨损}\\设备过早报废\end{tabular} \\
        \bottomrule
    \end{tabular}
\end{table}

表 \ref{tab:flash_constraints} 总结了嵌入式 Flash 介质的固有物理约束及其对快照性能的具体影响。这些“写敏感”特性决定了快照系统必须在软件层面对 I/O 模式进行重构，避免直接触发底层硬件的性能黑洞。

\subsection{内存环形缓冲区架构设计}

为了从根本上缓解上述问题，本研究在快照引擎与块设备驱动之间引入了一层中间件——写合并层。其核心数据结构是一个驻留在内核态的环形缓冲区，作为快照数据的“整形蓄水池”。该缓冲区的设计目标是实现数据的持续聚合和高效查找。该缓冲区并非简单的 \text{FIFO}（先进先出）队列，而是设计为具备地址索引功能的混合结构，包含以下关键组件：数据存储区，一段连续的物理内存，用于暂存待落盘的脏页数据；哈希索引表，以页面的物理地址（\text{PFN}）为键，以缓冲区内的槽位偏移量为值，这使得系统能够以 $O(1)$ 的时间复杂度查询某个物理页当前是否已存在于缓冲区中；以及脏页链表，维护当前缓冲区内所有有效脏页的写入顺序。这种结构确保了缓冲区具备了“在内存中原地更新数据”的能力，成为实现高效写合并的前提。


\subsection{逻辑覆写物理合并策略}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/ring.png}
    \caption{逻辑覆写物理合并策略}
    \label{fig:ring}
\end{figure}

利用上述数据结构，\text{HPRO} 系统实现了对高频更新页面的逻辑覆写，从而大幅减少了实际下发到磁盘的物理 \text{I/O} 次数。具体处理流程如下：

\textbf{极热页拦截与原地更新：} 在快照期间，某些页面可能会被业务进程反复修改，例如某页面 $P$ 在短时间内经历了 $V_1 \rightarrow V_2 \rightarrow V_3$ 三次状态变更。传统 \text{COW} 机制会依次触发三次 \text{I/O} 操作，分别写入 $V_1, V_2, V_3$。然而，在 \text{HPRO} 机制下，当 $V_1$ 到达缓冲区时，系统为其分配槽位并建立索引。当 $V_2$ 到达时，系统通过索引快速发现 $P$ 已存在，于是直接在内存中用 $V_2$ 覆盖 $V_1$ 的数据位置，无需进行任何物理 \text{I/O}。同理，$V_3$ 覆盖 $V_2$。最终，缓冲区中仅保留了该页面的最终状态 $V_3$。通过这种逻辑覆写，系统过滤掉了所有的中间态无效数据，将 $N$ 次逻辑写操作最终合并为单次物理 \text{I/O}。

\textbf{顺序化落盘：} 数据不会在缓冲区中无限驻留。落盘操作由两个条件触发：空间阈值（缓冲区使用率达到高水位，如 $80\%$）或时间阈值（由 \ref{sec:cont} 节中的动态增量策略控制的定时器超时触发）。触发落盘时，系统不再按照请求到达的随机顺序写入，而是根据脏页的物理地址（\text{LBA}）对缓冲区内的数据执行排序。排序后的数据被聚合成一个或多个大尺寸（如 $64\text{KB} - 512\text{KB}$）的顺序写入请求下发给 \text{Flash}。这种优化成功将对 \text{FTL} 极不友好的随机写转化为友好的顺序写，显著降低了写放大系数，延长了 \text{Flash} 的使用寿命。

图\ref{fig:ring}展示了这一过程：随机到达的脏页首先在环形缓冲区中通过哈希索引进行逻辑合并，过滤掉中间态数据；待缓冲区阈值触发后，剩余的有效页面被重组为对Flash友好的大块顺序写入请求，从而显著降低物理落盘频率。

\section{多层级冗余消除与压缩技术}

在嵌入式边缘虚拟化环境中，存储空间 ($S_{\text{storage}}$) 同样是极其稀缺的资源。无论是单次快照产生的镜像文件，还是连续快照产生的增量链，都包含了大量的、可消除的数据冗余。为了降低存储占用并进一步减少写入量，本研究引入了多层级冗余消除技术，这是对 $S_{\text{storage}}$ 约束的直接回应，结合全页去重与细粒度压缩，从逻辑层面削减 \text{I/O} 负载。

\subsection{基于指纹库的全页去重}

在边缘设备上，往往运行着多个同构的轻量级虚拟机或容器，它们共享相同的操作系统内核、基础库文件（如 \text{libc.so}, \text{libpython.so}）以及应用代码段\cite{barham2003xen, kivity2007kvm}。这些只读数据在内存中存在多份副本，是主要的冗余来源。

\text{HPRO} 系统实现了一个基于 MurmurHash3\cite{murmurhash3} 算法的全局指纹库。选择 \text{MurmurHash3} 是因为它具有极高的计算效率和低碰撞率，确保了在嵌入式 \text{CPU} 上执行哈希计算不会成为新的 $C_{\text{total}}$ 瓶颈。在写入任何页面之前，系统首先计算该页面的 128 位哈希值。系统在全局指纹库（通常采用开放寻址法的哈希表实现以优化内存占用）中查询该指纹。

如果查询命中，说明该数据内容在之前的快照或其他虚拟机中已存储过。系统仅在快照元数据中记录一个引用指针，格式为 $\langle\text{vm\_id}, \text{snapshot\_id}, \text{pfn}\rangle$，而不执行实际的数据写入。如果指纹未命中，则将该页面写入存储，并将新的指纹及存储位置添加到库中。这种全页去重机制通过消除跨机冗余和跨快照冗余，在多实例部署场景下能够实现可观的存储空间节省。

\subsection{细粒度提取}

全页去重只能处理完全一致的页面。然而，在实际运行中，许多脏页仅发生了微小的变化。例如，数据库更新一条记录时，可能只修改了 4\text{KB} 页面中的几十个字节，但传统机制不得不保存整个 4\text{KB} 页面。这种“大页微小修改”造成了严重的 \text{I/O} 浪费和存储资源浪费。

针对这一问题，本研究引入了细粒度提取机制。我们将标准的 4\text{KB} 页面逻辑划分为 $16$ 个 256\text{B} 的子块。在保存脏页时，系统不仅对比整页哈希，还会对这些子块的内容变化进行逐个比对。对于每个子块，系统检查其是否为全零，或者是否与上一版本的对应子块相同。

这种差异比对机制的核心在于实现紧凑存储与位图索引的精确协同。系统仅将发生实际变更的非零子块写入增量快照文件。同时，未变更的子块通过元数据中的位图进行标记跳过，而全零子块则通过特殊标记位记录，完全不占用任何物理存储空间。通过这种机制，逻辑写入粒度被从 4\text{KB} 成功缩小到 256\text{B}，从根本上解决了稀疏写入负载下的 \text{I/O} 浪费问题。正是这种 256\text{B} 的细粒度控制，使得 \text{HPRO} 系统在处理如 \text{SQLite} 的随机 \text{Key} 更新等负载时，能够显著提高存储效率并节省存储资源。

\section{基于 \text{SPI} 反馈的 \text{I/O} 流量整形}

在 \text{HPRO} 的差异化保存策略中，大量的冷寂页被安排在后台进行异步保存。虽然这是低优先级的任务，但在缺乏流控的情况下，后台迁移线程极易在短时间内占满 \text{Flash} 有限的写入带宽（通常仅为 $20\text{MB/s} - 40\text{MB/s}$）。这种 \text{I/O} 资源的独占会导致前台业务的关键 \text{I/O} 请求被迫在 \text{I/O} 调度器队列中排队，引发严重的“队头阻塞”，表现为业务响应延迟剧烈抖动。

为了保障前台业务的服务质量，本研究设计了基于令牌桶算法的 \text{I/O} 节流阀，并将其与第五章的 \text{SPI} 深度联动，实现了自适应的流量整形。

\subsection{令牌桶流控模型}

系统为后台快照线程设置了一个令牌桶结构，用于精确控制写入速率。该桶由三个参数定义：令牌生成速率 ($R_{\text{token}}$)、桶容量（决定了最大允许的突发写入量）和令牌消耗率。后台线程每写入一定量的数据，必须从桶中申请并消耗对应数量的令牌。如果桶中令牌不足，后台线程将被强制挂起（Sleep），进入等待状态，直到有新的令牌注入桶中。该机制充当了资源分配的门卫，将后台快照任务的瞬时速率限制在桶容量之下，并将其平均速率限制在 $R_{\text{token}}$ 之内，从而确保了快照 \text{I/O} 始终低于 $B_{\text{io}}$ 的安全阈值。线程的挂起和唤醒虽然涉及轻微的上下文切换开销，但在防止核心业务 \text{I/O} 发生拥塞方面，是成本效益最高的控制手段。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/bucket.png}
    \caption{SPI反馈式令牌桶流控}
    \label{fig:bucket}
\end{figure}

\subsection{\text{SPI} 驱动的动态速率调节}

传统的令牌桶采用固定的令牌生成速率，无法应对负载剧烈波动的边缘环境。本研究建立了令牌生成速率 $R_{\text{token}}$ 与系统压力指数 $\text{SPI}$ 之间的负相关反馈函数，实现了对 \text{I/O} 速率的动态自适应调整：

$$
R_{\text{token}} = R_{\text{max}} \times (1 - \text{SPI})
$$

该公式的动态调节逻辑构成了基于 \text{SPI} 的闭环控制系统，其控制目标是{最小化快照对业务 \text{I/O} 队列的贡献。当业务负载升高（$\text{SPI}$ 趋近 $1.0$）时，公式计算出的 $R_{\text{token}}$ 急剧下降甚至归零。此时，后台快照保存自动减速或完全暂停，将几乎全部 \text{I/O} 带宽让渡给前台业务，确保关键任务不受干扰，严格遵守 $B_{\text{io}}$ 约束。反之，当业务空闲（$\text{SPI}$ 趋近 $0$）时，令牌生成速率恢复至最大值 $R_{\text{max}}$。后台线程利用闲置带宽全速写入，以最快速度完成快照收尾。这种基于 \text{SPI} 的反馈机制，使得快照 \text{I/O} 流量能自适应地贴合系统的闲置资源曲线，从而在不引发队头阻塞的前提下，最大化快照效率。图\ref{fig:bucket}描绘了该反馈控制回路：SPI指数作为负反馈信号直接调节令牌生成速率。当业务繁忙导致SPI升高时，令牌生成自动收紧，迫使后台保存线程进入休眠，从而将I/O带宽优先让渡给前台关键业务。

\section{本章小结}

本章聚焦于嵌入式 \text{Flash} 存储介质的物理特性与性能瓶颈，构建了 \text{HPRO} 系统的底层执行优化层。首先，通过环形缓冲区写合并机制，利用地址哈希索引实现了逻辑覆写与顺序化落盘，将上层策略产生的高频随机逻辑写转化为低频的顺序物理写，从源头上降低了 \text{IOPS} 需求并显著缓解了写放大效应。其次，利用全局指纹库去重与细粒度压缩技术，最大限度地消除了跨快照、跨虚拟机及页内的冗余数据，显著节省了宝贵的存储空间并减少了写入磨损。最后，设计了基于 \text{SPI} 反馈的令牌桶流控，实现了后台快照流量对前台业务负载的自适应避让。这些底层优化措施与第四章的热点感知、第五章的动态调度共同构成了软硬协同的快照优化闭环，确保了上层的高效策略能够安全、稳定地落地于资源受限的物理硬件之上。至此，\text{HPRO} 系统的设计与实现论述完毕，下一章将通过实验数据验证其在真实环境下的性能表现。
