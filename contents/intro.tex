% !TEX root = ../main.tex

% 第一章一般名为绪论／引言，不可省略

\chapter{绪论}

\section{研究背景}

在当今数字化转型的宏观背景下，计算模式正在经历从集中式云计算向分布式边缘计算的深刻变革\cite{hamdaqa2012cloud, shi2022memory}。随着物联网（Internet of Things, IoT）、第五代移动通信技术（5G）以及人工智能（Artificial Intelligence, AI）技术的深度融合，全球计算范式正面临着从“集中式云端处理”向“分布式边缘计算”的结构性转变。在传统的云计算模型中，海量终端数据需回传至数据中心进行集中处理，这在面对自动驾驶、工业实时控制及增强现实（AR）等对时延敏感的应用场景时，往往面临带宽瓶颈、传输延迟过高以及数据隐私泄露等严峻挑战\cite{zhang2023review}。为了解决上述问题，计算能力开始大规模下沉至更贴近数据源头的网络边缘侧，形成了“云-边-端”协同的新型计算架构。在此背景下，边缘设备的角色发生了质的转变，不再局限于承担简单的数据采集与转发功能，而是演变为具备一定算力的智能计算节点\cite{shi2022memory}。特别是在深度学习算法轻量化技术的推动下，大量的 AI 推理任务，例如基于卷积神经网络（CNN）的视频监控分析、基于循环神经网络（RNN）的设备故障预测等，开始在边缘侧直接执行。这使得边缘计算环境下的工作负载表现出了显著的计算密集型与数据密集型特征，对底层系统软件的资源管理效率提出了更高的要求。

为了在异构且资源相对有限的边缘计算节点上同时运行多种类型的业务负载（例如同时运行实时操作系统 RTOS 与通用操作系统 Linux），并确保不同业务之间的安全隔离与故障自愈，虚拟化技术成为了构建边缘智能基础设施的关键支撑技术\cite{heiser2008role, aguiar2010embedded}。通过在硬件之上引入虚拟机监控器（Hypervisor），如 KVM（Kernel-based Virtual Machine）\cite{kivity2007kvm}或 Xen\cite{barham2003xen}，系统能够将物理资源抽象为多个独立的虚拟机（Virtual Machine, VM）或轻量级执行环境。在这一架构中，操作系统级的快照机制成为了保障系统高可用性（High Availability, HA）与可靠性的核心支撑技术\cite{ta2008virtual, dunlap2002revirt}。快照通过记录虚拟机在特定时刻的内存状态、CPU 寄存器上下文以及外设状态，为系统提供了“状态回滚”与“现场恢复”的能力。这对于边缘智能系统尤为重要：当运行关键 AI 决策任务的虚拟机出现软件故障或遭遇外部攻击时，系统必须能够在毫秒级时间内通过快照恢复至最近的一致性状态，以避免工业生产事故或服务长时间中断。

然而，尽管快照技术在资源充裕的数据中心服务器环境中已发展得相对成熟，形成了包括停止并拷贝（Stop-and-Copy）、预拷贝（Pre-copy）及后拷贝（Post-copy）在内的经典机制 \cite{clark2005live, hines2009post}。但在将其直接应用于边缘计算等资源受限环境时，面临着严峻的物理约束与适配挑战\cite{shi2019memory, zhong2016flic}。边缘计算节点与传统服务器在硬件架构上存在本质差异，边缘计算节点与传统服务器在硬件架构上存在本质差异，具体表现在以下四个维度：

（1） \textbf{计算资源受限}： 边缘设备通常搭载基于 ARM 或 RISC-V 架构的低功耗系统级芯片（SoC），其 CPU 核心数量较少且单核主频较低 。传统的快照过程涉及大量的内存页面扫描、脏页位图同步以及数据压缩计算。在算力匮乏的边缘节点上，这些管理操作会剧烈抢占业务虚拟机的 CPU 时间片，导致前台运行的 AI 推理任务出现严重的性能抖动，甚至因无法及时响应中断而导致实时性违约 。

（2） \textbf{存储介质的物理特性约束}：出于成本、体积与功耗的考虑，边缘节点普遍采用 NAND Flash 介质（如 eMMC、SD 卡或嵌入式 SSD）作为主存储设备 \cite{gobieski2019intelligence, zhang2023review}。与服务器级的 NVMe 阵列不同，这类存储介质的随机写入性能较差，且存在擦写寿命限制 \cite{zhong2016flic, shi2022memory}。传统快照技术往往倾向于高频、全量地将内存数据写入磁盘，这种粗放的写入模式不仅会迅速耗尽有限的 I/O 带宽，造成系统 I/O 阻塞，还会引发严重的写放大效应，加速 Flash 介质的物理磨损，从而缩短设备的使用寿命 。

（3） \textbf{内存容量约束}：边缘节点的物理内存容量通常在 MB 至 GB 级别，远低于服务器动辄 TB 级的内存配置 。在内存空间紧张的情况下，快照过程中产生的写时复制（COW）页面和缓冲区极易引发内存水位报警，导致操作系统内核频繁触发页面回收甚至 OOM（Out of Memory）杀手机制，严重影响系统的稳定性 。

（4） \textbf{能源消耗约束}：许多部署在野外的边缘节点依赖电池或能量收集装置供电 。快照过程中持续的高 CPU 负载与密集的 I/O 操作会产生显著的能耗。如何在保证数据安全的前提下，最小化快照操作的能量开销，是资源受限环境下必须考虑的关键指标。

综上所述，现有的面向数据中心的快照机制无法直接适应具有严格资源约束的边缘计算环境。如何设计一种能够感知底层硬件特性、适应边缘设备典型负载内存写入特征，并在计算、存储与能耗之间取得平衡的操作系统快照优化方案，已成为当前学术界与工业界亟待解决的重要课题。

\section{国内外研究现状}

\subsection{操作系统快照技术研究现状}

操作系统快照技术作为数据保护的第一道防线与系统容错的核心机制，其研究范畴已从早期的文件系统一致性备份扩展至现代的容器镜像管理、进程级检查点以及异构计算状态保存 。在文件系统层面，写时复制（Copy-on-Write, CoW）范式主导了技术演进。现代文件系统（如 ZFS 和 Btrfs）打破了传统原地更新的限制，通过操纵元数据指针实现了 $O(1)$ 复杂度的瞬间快照创建 \cite{bonwick2005zfs, rodeh2013btrfs}。虽然 ZFS 提供了成熟的事务模型，但在资源受限环境下，Btrfs 因其轻量级特性和对透明压缩、在线碎片整理的原生支持，表现出了更强的适应性 \cite{heger2014workload}。

随着云原生计算的普及，研究重点转向了容器镜像的分层存储与快速启动。针对传统 OCI 镜像解压效率低、冷启动延迟高的问题，学术界与工业界提出了基于块设备的远程镜像格式（如 OverlayBD）\cite{overlaybd_dadi}。该技术通过用户态块设备驱动将远程对象存储直接映射为本地虚拟块设备，实现了容器镜像的按需加载，将启动时间与镜像大小解耦，显著降低了网络开销与存储占用 \cite{overlaybd_dadi, overlaybd_practice}。在进程级快照领域，CRIU（Checkpoint/Restore In Userspace）已成为标准工具，最新的研究将其应用于 Serverless 计算场景，通过并发分页与工作集精简技术，解决了函数实例的冷启动延迟问题\cite{panda2023snapstore}。此外，随着 AI 负载的下沉，操作系统快照开始向异构硬件延伸。针对 GPU 状态难以保存的痛点，最新的研究提出了“验证式推测”机制，允许在 GPU 持续计算的同时进行显存数据的并发快照，消除了传统方案中昂贵的 API 拦截与停机开销，为大模型推理任务的实时迁移与容错提供了系统级支持 \cite{phoenixos2025, criugpu2025}。

\subsection{虚拟机快照技术研究现状}

% TODO: 承接上文

虚拟机快照技术位于虚拟化层的核心，旨在捕获包括内存、CPU 上下文及设备状态在内的完整系统现场。根据内存数据传输时机与系统停机策略的不同，经典的虚拟机快照机制主要分为三类：

\textbf{（1）停止并拷贝（Stop-and-Copy）}：这是最基础的快照机制。系统在快照开始时立即暂停虚拟机的执行，将全量内存状态一次性写入持久化存储\cite{dunlap2002revirt}，待写入完成后恢复运行。虽然该机制实现简单且能保证数据的绝对一致性，但其停机时间与内存大小呈线性正比关系。在 I/O 带宽受限的资源受限边缘计算平台上，保存 GB 级别的内存可能导致数十秒的服务中断，这对于实时性要求高的业务是不可接受的 。

\textbf{（2）预拷贝（Pre-copy）}：为了降低停机时间，预拷贝机制采用迭代传输策略\cite{clark2005live}。虚拟机在快照初期继续运行，Hypervisor 在后台将内存页面传输至目标端；同时，利用脏页追踪机制记录传输过程中被修改的页面（脏页），并在下一轮迭代中仅重传这些脏页。当剩余脏页量收敛到一定阈值时，系统短暂亦停机传输剩余数据。然而，在写密集型负载下，脏页的产生速率往往高于传输速率，导致迭代无法收敛\cite{akiyama2013fast}，系统不得不强制进入长时间停机，且迭代过程消耗大量 CPU 与 I/O 资源。

\textbf{（3）后拷贝（Post-copy）}：该机制采用“先停机，后传输”的策略\cite{hines2009post}。系统先暂停虚拟机，仅传输极少量的 CPU 状态（寄存器等），随后立即在目标端恢复虚拟机运行。内存数据则在后续运行过程中，通过缺页异常按需从源端拉取。虽然这极大地缩短了启动时的停机时间，但在计算能力孱弱的设备上，频繁的缺页异常处理（涉及内核态/用户态切换）会引入巨大的延迟，导致系统恢复初期的性能出现剧烈抖动 。

为了克服上述经典机制在性能与开销上的局限，近年来的研究主要集中在脏页追踪的硬件化与并行化。Intel 推出的 PML（Page Modification Logging）技术允许处理器将脏页物理地址直接记录到硬件日志缓冲区，仅在缓冲区满时触发异常，从而避免了传统影子页表或写保护机制引发的高频 VM Exit，显著降低了虚拟化层的 CPU 开销\cite{wang2014vm, intel_pml_arxiv}。此外，针对多 vCPU 大内存环境下的锁竞争问题，Linux 内核引入了脏页环（Dirty Ring）机制，为每个 vCPU 维护独立的环形缓冲区，极大提升了快照过程中脏页收集的并行度与效率 。针对微虚拟机（MicroVM）场景，Firecracker 等轻量级 Hypervisor 利用 mmap 技术实现快照文件的按需映射，支持了毫秒级的冷启动速度 。

\subsection{热点识别技术研究现状}

在增量快照与实时迁移过程中，盲目地复制所有数据是低效的。精准识别内存访问热点及估算工作集大小（Working Set Size, WSS），是制定内存分层、压缩与迁移决策的关键前提\cite{cui2018snapfiner}。传统的软件层面工作集估算方法（如基于空闲页追踪）需要频繁扫描页表，在大内存场景下会带来显著的 CPU 开销 。因此，研究重心逐渐转向基于硬件辅助的轻量级识别机制。现代处理器页表项中的脏位（Access Bit）与脏位（Dirty Bit）被广泛用于构建低开销的内存热图\cite{wu2016hot, cui2018snapfiner}。通过周期性扫描这些硬件标志位，系统能够以极低的代价捕捉内存访问的时空局部性特征，从而区分不同页面 。

除了识别热点页面，研究者还关注如何降低热点追踪的粒度以减少“伪共享”带来的无效传输。Intel 提出的子页级保护（Sub-Page Protection, SPP）技术将写权限控制粒度从 4KB 缩小至 128 字节，有效剔除了一页中未实际修改的数据块，显著降低了快照过程中的网络流量与存储写入量\cite{ozawa2021spp} 。此外，结合LRU\cite{o1993lru}等轻量级软件策略，系统能够模拟页面访问频率的时间衰减过程，更准确地预测快照期间的脏页分布，从而指导快照引擎在停机阶段优先处理高频修改的热点数据，避免预拷贝阶段的无效迭代 。

\subsection{资源受限环境快照研究现状}

随着边缘计算的兴起，快照技术的应用场景延伸至计算能力匮乏、I/O 带宽受限及能源敏感的边缘设备。在这一环境下，快照技术面临着严峻的“写放大”与存储寿命挑战\cite{zahed2009reducing} 。针对边缘设备普遍采用的 Flash 存储介质（如 eMMC、SD 卡），传统快照格式的随机写入模式会导致严重的性能下降与介质磨损。为此，研究者提出利用日志结构文件系统（如 F2FS）或日志结构合并树（LSM-Tree）来管理快照数据，将随机写入转化为顺序追加写，天然契合 NAND Flash 的物理特性\cite{hwang2015f2fs,dayan2018lsm}。最新的研究还进一步优化了文件系统的检查点机制，通过细粒度日志与数据/控制平面分离，显著降低了写停顿\cite{luo2018wisckey,zhong2016flic}。

在数据缩减方面，针对资源受限环境 CPU 较弱的特点，工业界倾向于采用写合并与后处理去重技术\cite{zhong2016flic}，即在内存缓冲区中合并对同一地址的多次覆写，仅将最终状态刷入 Flash，从而减少物理 I/O 次数 。对于边缘 AI 推理场景，大模型产生的 KV Cache 占用大量显存且重算昂贵，成为状态管理的新瓶颈。相关研究致力于将 KV Cache 视为一种特殊状态进行快照，并通过量化、稀疏化或卸载至主机内存等手段进行压缩持久化，以在有限的资源下实现高效的“有状态”推理与快速恢复\cite{gobieski2019intelligence, panda2023snapstore} 。此外，针对边缘网络不稳定的特点，基于应用感知的迁移机制被提出，通过解耦内存状态并仅同步必要的流状态，实现了弱网环境下的服务连续性保障\cite{shi2022memory, akiyama2013fast} 。

\section{研究挑战与主要研究内容}

随着边缘计算与人工智能技术的深度融合，计算节点逐渐演变为承载复杂逻辑的智能实体。然而，在将服务器级的虚拟化快照技术迁移至资源受限的边缘环境时，底层硬件的物理约束（计算弱、带宽低、存储寿命短）与上层 AI 负载的运行特征（高频更新、时空局部性强）之间产生了剧烈的冲突。这种“软硬不匹配”不仅体现在算力与带宽不足，还体现在虚拟化管理层开销在低频 CPU 上被成倍放大、Flash 存储的写放大在频繁的脏页传输中迅速恶化、以及 AI 推理任务对延迟敏感的负载模式与快照过程中不稳定的数据路径发生直接冲突。上述因素叠加，使得传统快照机制在边缘环境下出现明显的性能退化、不可控的停机窗口以及不稳定的业务干扰。具体挑战可概括为以下三个方面：

\textbf{挑战1：经典快照机制在异构负载下的选型困境与失效}

在快照技术路线的选型上，现有的三大主流机制——停止并拷贝、预拷贝与后拷贝在面对边缘场景时均陷入了性能与可用性的两难境地，每一种方法在资源受限环境下均暴露出其设计假设失效的问题。

\begin{itemize}

\item \textbf{停止并拷贝的不可接受性}：
停止并拷贝机制要求虚拟机完全暂停直至所有内存落盘，整个执行路径为严格的“停机→保存→恢复”。在 I/O 带宽仅有数十 MB/s 的资源受限边缘计算平台上，保存 1GB 内存可能导致数秒级的服务中断。此外，AI 推理或工业控制任务通常具备严格的实时性约束，无法容忍如此长的不可用窗口，即便短暂的停顿也可能导致推理链路断裂、控制环闭合失败或实时任务无法按周期响应。因此，在边缘场景直接应用停止并拷贝机制几乎不存在可行性。

\item \textbf{预拷贝的收敛性崩溃}：
预拷贝机制的核心依赖是假设“传输速率大于脏页产生速率”。然而在写密集型负载（如 AI 推理过程中反复更新的特征图缓冲区、数据库事务中的日志区域）中，边缘设备的脏页产生速率往往远高于网络或 Flash 写入速率，使得迭代阶段持续产生新的脏页，导致迭代无法收敛。为了强制结束快照，系统不得不提前进入停机阶段，造成长时间暂停。同时，迭代过程中伴随的大量脏页扫描、位图同步与 I/O 传输会进一步抢占有限的 CPU 与带宽，使得原本用来减少停机时间的机制反而放大了整体开销。

\item \textbf{后拷贝的性能损失风险}：
后拷贝机制虽可在毫秒级时间内完成快照（仅保存 CPU 状态并恢复执行），但后续依赖缺页异常按需拉取内存。对于高主频多核服务器，这种缺页处理开销相对可控；但在计算能力孱弱、虚拟化陷入成本高的嵌入式 CPU 上，频繁的缺页异常会引发大量 VM Exit，使得系统在恢复初期面临成百上千次的上下文切换，出现大幅性能抖动。对于对延迟极其敏感的 AI 或控制任务，这种抖动意味着不可接受的服务中断。因此，如何设计一种既能获得后拷贝的低停机时间优势，又能避免缺页风暴的混合式快照策略，是本研究面临的首要且基础性的挑战。

\end{itemize}

\textbf{挑战2：虚拟化脏页追踪的高开销与热点预测的滞后性}

在资源受限环境下，脏页追踪的成本被显著放大。在虚拟化平台中，快照过程不仅面临硬件资源受限带来的直接挑战，还受到虚拟化层本身的运行机制影响。

传统的脏页追踪依赖于写保护策略：当虚拟机写入被保护页面时，硬件会触发扩展页表违例（EPT Violation）或 Stage-2 Fault，从而导致一次 VM Exit，使 CPU 在宿主机与虚拟机模式间进行昂贵的上下文切换。在高性能服务器上，这类开销相对可控，但在主频较低、核数有限的边缘设备或嵌入式处理器上，频繁的 VM Exit 会显著侵蚀可用于业务计算的时间片，直接导致前台 AI 推理或实时任务出现延迟抖动甚至周期违约\cite{wang2014vm}。此外，KVM 与 QEMU 协作的用户态脏页收集机制需要 QEMU 频繁通过系统调用同步脏页位图，在资源紧张的环境中进一步放大了系统调用开销，使得快照过程在 CPU 使用率方面成为前台任务的可观竞争者。

与此同时，准确识别内存热点是减少无效数据传输、提升快照效率的基础，但在资源受限环境中实现高精度热点识别本身也极具挑战性。AI 工作负载的内存写入通常具有强烈的时空局部性，同时呈现阶段性漂移特征，即热点区域会随着推理阶段、数据流变化或模型执行路径不同而动态迁移\cite{wu2016hot, cui2018snapfiner}。如果采用 LRU 等传统策略，维护大量页面链表会造成显著的内存与 CPU 管理开销，这在资源受限设备上几乎不可接受\cite{wang2014vm}；而若采用粗粒度的访问追踪方法，又会导致热点识别精度不足，大量本不活跃的页面被误判为热点，进而在预拷贝或后台传输阶段被频繁搬运，加剧 I/O 带宽消耗和写放大问题\cite{zhong2016flic}。虚拟化层的 VM Exit 开销叠加热点识别的不精确性，使得传统快照机制在此类场景下不仅难以收敛，还可能长时间占用系统资源，引发业务性能剧烈抖动，严重影响实时性与可靠性。

\textbf{挑战3：嵌入式 Flash 存储特性的物理约束与写放大效应}

边缘节点普遍采用 NAND Flash（如 SD 卡、eMMC）作为主存储，这类基于浮栅电荷存储的 Flash 介质无论在架构还是性能特性上，都与数据中心使用的 NVMe SSD 存在本质差异\cite{aguiar2010embedded}。

Flash 存储天然对顺序写入更为友好，但其随机写入性能显著低于顺序写；同时，每个块在写入前必须执行擦除操作，且 P/E循环次数有限，使得写入寿命成为必须考虑的关键因素。在快照过程中，若对内存页面不加区分地执行全量落盘，系统会持续产生大量随机、离散且短周期的写入请求，不仅会迅速耗尽受限的 I/O 带宽导致业务线程出现长时间阻塞甚至系统“假死”，还会进一步放大 Flash 原有的写放大效应，使得同一物理块被频繁擦写，加速磨损并显著缩短设备寿命\cite{zahed2009reducing}。

此外，在资源受限的边缘节点上，快照过程中的密集写入流与前台业务的正常 I/O 请求共享同一条受限的存储带宽通路；缺乏有效流控与优先级调度机制时，后台快照线程可能会快速占满所有可用 I/O 通道，使得业务关键路径上的读写请求被迫排队甚至超时，从而出现业务延迟飙升、网络堆积、数据库响应抖动等严重后果\cite{shi2019memory, panda2023snapstore}。尤其在 AI 推理或实时控制等对延迟敏感的场景中，I/O 饥饿问题将导致推理周期无法按时完成或执行控制逻辑的任务错失其实时性窗口。因此，Flash 存储特性的限制不仅影响快照本身的效率，更直接影响边缘节点整体系统的稳定性、可用性与长期可靠性，这也使得针对 Flash 特性的存储优化与 I/O 资源协调成为快照机制在边缘环境中不可回避的关键挑战。

针对上述挑战，本研究并没有局限于对现有技术的修修补补，而是重新审视了快照流程中的关键决策点，提出并实现了一种基于“热点感知（Hotspot Perception）”与“资源反馈（Resource Feedback）”的操作系统快照优化框架HPRO（Hotspot Perception and Resource Optimization）。该框架的核心思想是：在传统快照机制的基础上引入软硬协同的热点感知逻辑，通过实时理解虚拟机内存写入的局部性特征与系统资源剩余水位，动态地调节快照执行策略，从而同时降低停机时间、减少对 I/O 的突发压力并显著提升整体快照效率。与以往静态、不可调节、对负载模式不敏感的快照设计不同，本研究强调“感知 + 决策 + 协同 + 治理”的全链路动态优化，从体系结构层面重构快照的工作方式，使其能在资源受限环境中稳定工作。本文具体研究内容如下。

\textbf{1. 基于热点感知与资源反馈的快照系统架构}

本研究提出了一个在资源受限场景下面向虚拟化快照的分层感知与反馈控制架构HPRO,该架构通过将“监控—决策—执行”解耦，并采用闭环反馈式的系统设计，使快照能够在复杂负载与有限资源条件下实现自适应调节。

\textbf{（1）内核监控层}

位于 KVM 宿主机内核态，负责采集快照优化所需的底层运行信息。该层利用硬件辅助的脏位机制获取内存写入热点，并通过共享内存将采样结果无锁地传递给用户态模块，从而避免传统写保护与系统调用带来的额外开销。

\textbf{（2）策略决策层}

运行于 QEMU 用户态，是 HPRO 的调度核心。该层基于底层上报的访问信息构建内存热度分布，并结合 CPU 负载、I/O 队列深度与内存水位等系统指标计算系统压力指数（SPI）。通过 SPI 的反馈，策略层可动态调节快照执行模式与写入节奏，实现对资源状态的自适应响应。

\textbf{（3）执行优化层}

负责具体的数据搬迁与存储写入。根据策略层输出的指令，该层对不同热度分类的页面采用差异化处理，并集成写合并、去重及 I/O 限流等优化功能，以减少 Flash 随机写入、降低写放大并避免对前台业务造成干扰。

通过上述三层的协同工作，HPRO 架构构建了从底层感知到策略调整再到执行优化的完整闭环，使虚拟化快照在资源受限环境中能够根据实时负载变化实现自适应、高效且业务友好的运行方式。

\textbf{2. 软硬协同的内存写入热点感知机制}

为在资源受限设备上实现高精度、低开销的热点识别，本研究提出一种软硬协同的轻量化方案。首先，利用现代处理器页表项（PTE）中由硬件自动置位的脏位（Dirty Bit），系统能够在不产生 VM Exit 并且无需额外异常处理的前提下捕获页面的写入行为。通过周期性扫描和清除脏位，可以以极低成本生成当前内存写入分布，从而避免传统写保护机制带来的频繁陷入开销。在硬件辅助的基础上，本研究设计了自适应多位老化算法。该算法仅为每页分配 2 bit 的状态，通过位移与更新操作反映页面在多个时间窗口内的访问模式，使页面能够被划分为“极热、温热、冷寂”三个热度等级。这种结构简单、状态转换明确的模型能够在不显著增加元数据维护成本的情况下，提供足够的区分能力以支持快照阶段的热点优先策略。此外，本研究针对预拷贝过程中热点漂移的特性，提出两阶段工作集感知策略。该策略在快照的不同阶段采用不同的判定规则：在初始阶段利用空间局部性宽松选取潜在热点，在中间阶段逐步过滤非活跃区域，而在后期阶段根据访问减少趋势对候选集合进行衰减，确保工作集稳定收敛。这种分阶段的策略能够在保持判定准确性的同时避免不必要的反复迭代，提高预拷贝阶段的收敛可能性。基于上述机制，本研究实现的热点感知体系在不修改虚拟机内部结构、不引入高开销中断的前提下，为快照过程提供可依赖的“最小化脏页集”基础。

\textbf{3. 基于SPI的自适应快照决策模型}

传统快照机制多使用固定策略，忽略了快照任务与前台业务在资源使用上的动态竞争。本研究引入系统压力指数（SPI）作为统一量化指标，用于反映当前设备对快照任务的承载能力。SPI 通过整合 CPU 负载、I/O 队列深度、内存可用水位及电池电量等物理量，在不同负载阶段为快照系统提供分级反馈依据。依托 SPI，本研究构建了三种执行模式：高压模式、标准模式与低压模式。在高压模式下，系统优先保障业务执行，通过提升热点阈值减少需要立即处理的页面数量，并限制后台迁移带宽，以降低快照对前台任务的影响。在标准模式下，系统在保证业务稳定的前提下尽可能推进快照进程。在低压模式中，则充分利用可用资源，扩大预拷贝范围，以减少后续可能出现的欠页中断，提高整体执行效率。此外，本研究通过 SPI 指导动态增量策略。系统根据当前负载自适应调整增量快照的批处理大小：在高负载阶段，增大批处理规模可以减少随机写操作，降低 Flash 写放大；在低负载阶段，减小批处理规模有利于提升快照实时性。该策略结合 SPI 的实时反馈，使快照过程能够平衡延迟、带宽和资源占用。总体而言，SPI 决策模型通过对系统资源使用状况的实时感知，使快照过程具备动态调节能力，避免传统机制在复杂负载下出现大幅抖动。

\textbf{4. 面向Flash特性的存储优化与I/O治理}

针对边缘环境普遍使用 NAND Flash（如 SD 卡、eMMC）作为主存储的特点，本研究从 Flash 写入特性与带宽竞争问题出发，对快照写入路径进行了全链路优化。首先，本研究基于“极热区域高频写”的特性提出写合并机制（Write Coalescing）。通过引入环形缓冲区，将高频更新的页面数据在内存中进行集中处理，仅在条件满足时一次性落盘，从而减少对 Flash 的随机写入次数并降低写放大效应。其次，利用 MurmurHash3\cite{murmurhash3} 构建的全局指纹库，本研究实现了页面级去重，并进一步支持 256B 子页级别的细粒度压缩，使得仅发生变更的数据块被写入。该方案在不增加复杂性或额外存储开销的前提下有效降低了快照数据量。最后，本研究采用与 SPI 联动的令牌桶机制对 I/O 进行协调。在系统压力较高时，后台迁移线程因令牌不足而自动减速甚至暂停，避免对前台业务造成 I/O 饥饿。在系统压力较低时，传输速率可自动提升，以加速快照完成。该策略在保证业务连续性的同时提高 Flash 利用效率。

最后，通过在典型资源受限平台树莓派上进行的系统性实验与评估，全面证实了 HPRO 架构在边缘计算场景下的有效性，并展现了其在降低业务性能干扰、缩短停机时间以及减少存储写放大方面的显著优势。综上所述，本研究采用资源约束建模、关键技术攻关与原型系统验证的三重递进研究框架，系统性地解决了“感知滞后”、“资源死锁”与“介质磨损”三个核心挑战，形成了从底层硬件感知到上层策略调度的完整闭环解决方案。本文的总体研究框架与技术路线如图\ref{fig:struct}所示，涵盖了从问题定义、关键技术攻关到系统验证的全过程。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/struct.png}
    \caption{本文研究框架}
    \label{fig:struct}
\end{figure}

\section{论文结构安排}

本文共分为八章，文章的逻辑结构遵循“提出问题—分析问题—解决问题—验证效果”的学术范式，每个章节的具体内容安排如下：

第一章为绪论部分，主要介绍了人工智能与边缘计算融合发展的宏观背景，剖析了传统虚拟化快照技术在资源受限环境下面临的“水土不服”现象，从而确立了本研究的核心动机。同时，明确了研究面临的关键技术挑战，并提出了基于热点感知与资源反馈的研究思路。此外，还系统回顾了国内外在操作系统快照、脏页追踪及边缘存储优化等领域的研究现状，为后续研究奠定了理论与实践基础。

第二章梳理了本研究所涉及的核心技术背景与底层机制。首先，阐述了面向边缘计算的轻量级虚拟化架构，分析了 CPU、内存及 I/O 虚拟化在资源受限环境下的运行特征；其次，深入剖析了停止并拷贝、预拷贝及后拷贝三种主流虚拟机快照机制的运行原理，揭示了它们在写密集型负载与弱 I/O 带宽约束下的失效模式；再次，探讨了内存脏页追踪技术的演进路线，对比了传统软件写保护机制与现代硬件辅助特性的性能差异；最后，分析了嵌入式 Flash 存储介质的读写非对称、异地更新及写放大等物理特性，为后续的系统优化设计提供了必要的理论支撑与物理依据。

第三章构建了嵌入式边缘虚拟化环境下的理论基础与系统架构。首先建立了资源受限环境的数学约束模型，量化了计算、带宽及能源的边界条件；其次，深入分析了内存脏页追踪技术的底层机制与局限性；最后，提出了 HPRO（Hotspot Perception and Resource Optimization）系统总体架构，阐明了内核监控层、策略决策层与执行优化层之间的协同逻辑，为后续各功能模块的设计提供顶层指导。

第四章聚焦于 HPRO 架构中的“感知层”设计，重点阐述了轻量级热点识别机制。针对传统写保护机制开销过大的问题，提出了基于硬件辅助脏位的自适应多位老化算法，并设计了两阶段工作集感知策略，旨在以极低的系统损耗实现对内存写入时空局部性的精准捕捉与热点分级。

第五章围绕 HPRO 架构中的“决策层”展开，构建了基于资源反馈的动态快照策略。本章详细定义了系统压力指数（SPI）的计算模型，阐述了如何利用 SPI 实时感知 CPU 负载、I/O 队列深度及内存水位，并设计了自适应的状态机与动态批处理机制，实现了快照执行模式在“业务优先”与“快照优先”之间的智能切换。

第六章致力于解决 HPRO 架构中的“执行层”问题，即面向 Flash 特性的存储与 I/O 优化。针对嵌入式 Flash 介质的物理缺陷，设计了基于环形缓冲区的写合并机制与细粒度去重方案，并引入了反馈式令牌桶流控算法，从根本上缓解了快照过程中的写放大效应与 I/O 阻塞问题。

第七章通过一系列对比实验与量化评估，多维度验证了 HPRO 系统在资源受限环境下的性能表现。实验涵盖了不同负载类型（如 AI 推理、数据库读写等）下的停机时间、业务性能损耗及存储空间占用等关键指标，充分证明了本研究提出方法的有效性与先进性。

第八章对本研究的主要成果与创新点进行了总结，并针对当前工作中存在的局限性（如热点预测的滞后性、异构内存支持不足等），展望了未来在新型内存（CXL等）适配及全系统一致性保存方向的研究前景。

\section{本章小结}

本章为整个研究奠定了背景基础，深入剖析了人工智能与边缘计算深度融合背景下，嵌入式边缘虚拟化技术作为边缘智能底座的重要性，以及传统服务器级快照机制在资源受限环境下的性能退化与适应性不足 。从而明确了在边缘侧资源受限场景中，研究基于热点感知与资源反馈的快照优化技术的必要性与价值，并确立了本研究的核心动机 。通过对虚拟化脏页追踪高开销、快照收敛困难及 Flash 存储写放大等关键挑战的系统分析 ，以及对国内外在虚拟机快照、内存追踪及边缘存储优化领域现有研究成果的全面回顾 ，本文进一步明确了研究内容，即探索在资源受限的虚拟化环境中，如何构建软硬协同的热点识别机制、资源自适应的动态决策模型以及面向 Flash 特性的全链路 I/O 优化方案 ，旨在为构建高可用、长寿命的边缘智能基础设施提供新的思路与技术支持 。

